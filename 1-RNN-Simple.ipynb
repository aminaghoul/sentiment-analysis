{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F4YxwviWtesV"
   },
   "source": [
    "# RNN pour l'analyse de sentiments\n",
    "\n",
    "Nous allons maintenant faire une analyse de sentiments sur le même jeu de données en utilisant les réseaux de neurones récurrents\n",
    "\n",
    "## Réseaux de neurones récurrents\n",
    "\n",
    "Les réseaux de neurones récurrents ou (RNN), sont souvent utilisés pour analyser des séquences.\n",
    "\n",
    "En effet, dans les réseaux de neurones généraux, un input est traité par un certain nombre de couches et un output est produit à la sortie, avec l'hypothèse que deux inputs successifs sont indépendants.\n",
    "\n",
    "Cependant, cette hypothèse n'est pas correcte dans un certain nombre de scénarios. \n",
    "Par exemple, si on veut prédire le mot suivant dans une séquence, il est indispensable de considérer la dépendance des observations précédentes.\n",
    "    \n",
    "Dans notre cas, le modèle RNN prend une séquence de mots $X=\\{x_1, ..., x_T\\}$, une à la fois, et produit un état caché $h$, pour chaque mot.\n",
    "On utilise le RNN en lui donnant le mot courant $x_t$ ainsi que l'état caché du mot précédent, $h_{t-1}$, pour produire l'état caché suivant, $h_t$.\n",
    "\n",
    "\n",
    "$$h_t = \\text{RNN}(x_t, h_{t-1})$$\n",
    "\n",
    "\n",
    "Une fois que l'on a notre état caché final, $h_T$, obtenu après avoir donné le dernier mot de la séquence $x_T$ au modèle, on le donne à une couche linéaire $f$, (qui s'appelle également fully connected layer), pour recevoir notre sentiment prédit, $\\hat{y} = f(h_T)$.\n",
    "\n",
    "<center> <img src=\"RNN.png\" alt=\"drawing\" width=\"700\"/>\n",
    "        \n",
    "        \n",
    "Cette illustration montre un exemple de phrase, avec le RNN prédisant 0, c'est-à-dire que le sentiment est négatif. Le RNN est représenté en orange et la couche linéaire est en gris. On utilise le même RNN pour chaque mot, c'est-à-dire qu'il a les mêmes paramètres. L'état initial caché $h_0$, est un tensor initialisé à zéro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IINy-8uQtesY"
   },
   "source": [
    "## Préparation des données\n",
    "\n",
    "Dans ce notebook, on utilise la librairie torch et TorchText.\n",
    "\n",
    "TorchText a une méthode `Field` qui sert à définir comment les données brutes doivent être traitées.\n",
    "\n",
    "La méthode `TEXT` définit comment les commentaires doivent être traités, et `LABEL` comment les labels doivent être traités. \n",
    "\n",
    "`TEXT` a un argument `tokenize`, qui par défaut split les chaînes de caractères en espaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SV06LPyQtesa"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "TEXT = data.Field(sequential=True,lower=True, tokenize = 'spacy')\n",
    "\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On sépare les données en train et test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille des données train: 18163\n",
      "Taille des données de validation: 2270\n",
      "Taille des données test: 2271\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
    "        path='./data/', train='train.csv',\n",
    "        validation='valid.csv', test='test.csv', format='csv', skip_header=True,\n",
    "        fields=[('text', TEXT), ('label', LABEL)])\n",
    "\n",
    "print(f'Taille des données train: {len(train_data)}')\n",
    "print(f'Taille des données de validation: {len(valid_data)}')\n",
    "print(f'Taille des données test: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NOCX0qGNtesw"
   },
   "source": [
    "On affiche un exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "XXxXnQwctesx",
    "outputId": "bca3747b-655d-4ae7-fb2f-29b49a373eb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['this', 'movie', 'is', 'a', 'gem', 'because', 'it', 'moves', 'with', 'soft', ',', 'but', 'firm', 'resolution.<br', '/><br', '/>i', 'caution', 'viewers', 'that', 'although', 'it', 'is', 'billed', 'as', 'a', 'corporate', 'spy', 'thriller', 'and', 'ms', 'liu', 'is', 'there', ',', 'it', 'moves', 'at', 'a', 'deftly', 'purposeful', 'yet', 'sedate', 'pace', '.', 'it', \"'s\", 'not', 'about', 'explosions', ',', 'car', 'chases', ',', 'or', 'flying', 'bullets', '.', 'you', 'must', 'be', 'patient', 'and', 'instead', ',', 'note', 'the', 'details', 'here', '.', 'it', \"'s\", 'sedate', 'because', 'that', \"'s\", 'what', 'the', 'main', 'character', 'is', '.', 'the', 'viewer', 'has', 'to', 'watch', 'him', 'and', 'think', 'as', 'this', 'story', 'unfolds.<br', '/><br', '/>i', 'will', 'not', 'give', 'spoilers--', 'because', 'that', 'destroys', 'the', 'point', 'of', 'watching', '.', 'the', 'plot', 'is', 'what', 'you', \"'ve\", 'read', 'from', 'the', 'other', 'postings', ':', 'an', 'average', 'white', '-', 'collar', 'guy', ',', 'seeking', 'change', 'and', 'adventure', ',', 'signs', 'on', 'for', 'a', 'corporate', 'spy', 'job', '.', 'just', 'go', 'somewhere', 'and', 'secretly', 'record', 'and', 'transmit', 'inside', 'data', '.', '<', 'br', '/><br', '/>take', 'it', 'from', 'there.<br', '/><br', '/>this', 'movie', 'starts', 'at', 'a', 'surreal', 'walk--', 'with', 'a', 'background', 'tang', 'of', 'corporate', 'disillusionment', 'that', 'entwines', 'itself', 'with', 'quintessential', ',', 'underlying', 'suburban', 'paranoia.<br', '/><br', '/>then', 'it', 'begins', 'to', 'accelerate.<br', '/><br', '/>the', 'acting', 'on', 'all', 'parts', 'is', 'superb--', 'and', 'yes', ',', 'some', 'of', 'the', 'acts', 'are', 'caricature', 'characters', '.', 'but', 'they', 'all', 'fit', ',', 'and', 'they', 'entertain', '.', 'and', 'the', 'light', 'piano', 'rhyme', 'in', 'the', 'background', 'is', 'just', 'perfect', 'as', 'the', 'soft', ',', 'soft', 'key', 'sinister', 'theme', ':', 'all', 'is', 'not', 'right', 'at', 'the', 'beginning.<br', '/><br', '/>and', 'at', 'the', 'end', ':', 'all', 'is', 'not', 'what', 'it', 'seems.<br', '/><br', '/>get', 'comfortable', 'and', 'turn', 'the', 'lights', 'down', 'to', 'watch', 'this', 'one--', 'and', 'turn', 'up', 'the', 'sound', ':', 'this', 'movie', 'wants', 'you', 'to', 'listen', '.'], 'label': '0'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V_a2QAqztes7"
   },
   "source": [
    "On crée un échantillon de données de validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2JwQ2BY0tetK"
   },
   "source": [
    "On crée un vocabulaire sur l'échantillon train avec la méthode `build_vocab`et on ne prend que 25000 mots pour diminuer la dimension de la matrice d'embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j8gQvkW7tetL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de tokens unique dans le TEXT: 10002\n",
      "Nombre unique de LABEL: 2\n"
     ]
    }
   ],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "print(f\"Nombre de tokens unique dans le TEXT: {len(TEXT.vocab)}\") \n",
    "print(f\"Nombre unique de LABEL: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H-EuaMfGtetU"
   },
   "source": [
    "Les deux valeurs supplémentaires dans le vocabulaire du TEXT sont les tokens `<unk>` et `<pad>`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "m0yMEfHptetW",
    "outputId": "b4503fa3-92e7-479b-c64b-79797e8ad0db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 274223), (',', 239861), ('.', 204258), ('and', 135511), ('a', 135466), ('of', 121892), ('to', 112826), ('is', 92798), ('in', 77926), ('it', 74198), ('i', 60943), ('that', 60269), ('this', 55592), ('\"', 54672), (\"'s\", 54540), ('-', 45599), ('\\n', 42455), ('as', 39521), ('with', 37130), ('was', 36791)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nXX3_jXLtetn"
   },
   "source": [
    "Les valeurs du vocabulaire de LABEL sont 0 pour positif et 1 pour négatif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "P4i4-Fhjteto",
    "outputId": "9e71e222-9875-436f-e807-2d231626177a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'0': 0, '1': 1})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j8ovGiuJtett"
   },
   "source": [
    "La dernière étape de la préparation des données consiste à créer les itérateurs. Nous les parcourons dans la boucle d'apprentissage / d'évaluation, et ils retournent un lot d'exemples (indexés et convertis en tenseurs) à chaque itération.\n",
    "\n",
    "On utilise `BucketIterator` qui est un itérateur qui renverra un lot d'exemples où chaque exemple est d'une longueur similaire, minimisant la quantité de padding par exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vx_Fb_59tetu"
   },
   "outputs": [],
   "source": [
    "# utilisation du GPU si possible \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device, sort = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tz0537L1tetx"
   },
   "source": [
    "## Construction du modèle\n",
    "\n",
    "Le modèle RNN utilisé se compose des couches suivantes : \n",
    "\n",
    "- _embedding_ : utilisé pour transformer notre vecteur one hot encoder (dont la plupart des éléments sont des 0) en un vecteur embedding dense (dense car la dimensionnalité est beaucoup plus petite et tous les éléments sont des nombres réels). De plus, les mots qui ont un impact similaire sur le sentiment de la revue sont mappés de manière rapprochée dans cet espace vectoriel dense.\n",
    "- _RNN_ : prend le vecteur dense et le précédent état caché $h_{t-1}$, et calcule l'état caché suivant $h_t$.\n",
    "- _linéaire_ : prend le dernier état caché, le met dans un couche fully connected $f(h_T)$ qui le transfome en output prédit.\n",
    "\n",
    "On appelle la méthode `forward` lorsque l'on donne nos exemple à notre modèle.\n",
    "\n",
    "Chaque batch, `text` est un tensor de dimension _**[sentence length, batch size]**_. C'est un batch de commentaires, dont les mots sont convertis en représentation one-hot encoder. PyTorch stocke  un vecteur one-hot comme sa valeur d'index.\n",
    "\n",
    "Le batch de l'input est ensuite passé à travers la couche embedding pour obtenir une représentation de vecteur dense de nos commentaires.\n",
    "`embedded` est un tensor de taille _**[sentence length, batch size, embedding dim]**_.\n",
    "\n",
    "`embedded`est ensuite introduit dans la couche RNN. Dans certains frameworks, vous devez alimenter l'état caché initial, $h_0$, dans le RNN, cependant dans PyTorch, si aucun état caché initial n'est passé comme argument, il prend par défaut un tenseur de valeurs zéros.\n",
    "\n",
    "The RNN retourne 2 tenseurs, `output` de taille _**[sentence length, batch size, hidden dim]**_ et `hidden` de taille _**[1, batch size, hidden dim]**_.\n",
    "\n",
    "`output` est la concaténation de l'état caché de chaque pas de temps, alors que `hidden` est l'état caché final. \n",
    "La méthode `squeeze` qui est utilisé pour supprimer une dimension de taille 1. \n",
    "\n",
    "Enfin, nous alimentons le dernier état caché, `hidden`, à travers la couche linéaire, `fc`, pour produire une prédiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O8BPYwRGtety"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim,bidirectional):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, bidirectional=bidirectional)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "\n",
    "        embedded = self.embedding(text)\n",
    "               \n",
    "        output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        \n",
    "        return self.fc(hidden)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1KBgfaFitet5"
   },
   "source": [
    "On crée alors notre modèle.\n",
    "\n",
    "La dimension de l'input est la taille du vocabulaire. \n",
    "\n",
    "La dimension de l'embedding est la taille des vecteurs denses. Elle est généralement de 50 à 250, mais elle dépend de la taille du vocabulaire.\n",
    "\n",
    "La dimension de hidden est la dimension des états cachés. Elle vaut généralement 100 à 500, mais dépend de la taille du vocabulaire, des vecteurs denses et la compléxité de la tâche. \n",
    "\n",
    "La dimension de l'output est le nombre de classes, ici, la valeur de l'output est entre 0 et 1 et donc est de dimension 1 car l'output est un scalaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3A_TUXSftet8"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "BIDIRECTIONAL = True\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,BIDIRECTIONAL )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DHhyWBk3teuA",
    "outputId": "9586cff6-25a5-49cf-c85f-771a20ee6a29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le modèle a 1,184,009 paramètres à entraîner\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'Le modèle a {count_parameters(model):,} paramètres à entraîner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zGjJ1BSSteuF"
   },
   "source": [
    "## Entraîner le modèle\n",
    "\n",
    "On entraîne le modèle.\n",
    "On utilise l'optimiseur Adam qui ne prend en compte seulement comme paramètres les paramètres du modèle qui seront actualisés par l'optimiseur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MC34zFI4teuI"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ldK_vHILteuN"
   },
   "source": [
    "On définit ensuite la fonction de perte qui est ici la _binary cross entropy with logits_. \n",
    "\n",
    "Notre modèle génère actuellement un nombre réel non lié. Comme nos étiquettes sont 0 ou 1, nous voulons limiter les prédictions à un nombre entre 0 et 1. Nous le faisons en utilisant les fonctions _sigmoid_ ou _logit_.\n",
    "\n",
    "Nous utilisons ensuite ce scalaire lié pour calculer la perte en utilisant l'entropie croisée binaire.\n",
    "\n",
    "Le critère `BCEWithLogitsLoss` effectue à la fois les étapes d'entropie croisée sigmoïde et binaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bEn4T95ZteuN"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NpprToLYteuQ"
   },
   "source": [
    "En utilisant `.to`, nous pouvons placer le modèle et le critère sur le GPU (si nous en avons un)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kh294K1JteuR"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w42XP6EbteuV"
   },
   "source": [
    "Notre fonction de critère calcule la perte, cependant nous devons écrire notre fonction pour calculer l'accuracy.\n",
    "\n",
    "Cette fonction alimente d'abord les prédictions à travers une couche sigmoïde, en écrasant les valeurs entre 0 et 1, nous les arrondissons ensuite à l'entier le plus proche. Cela arrondit toute valeur supérieure à 0,5 à 1 (un sentiment positif) et le reste à 0 (un sentiment négatif).\n",
    "\n",
    "Nous calculons ensuite le nombre de prédictions arrondies égales aux labels réels et la moyenne sur l'ensemble du batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4G8cW2IpteuX"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Retourne l'accuracy par batch\n",
    "    \"\"\"\n",
    "    #arrondi la prédiction à l'entier le plus proche\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def precision(preds, y):\n",
    "    '''\n",
    "    Retourne la précision\n",
    "    '''\n",
    "    y_pred = torch.round(torch.sigmoid(preds))\n",
    "    y_true = (y_pred == y).float() \n",
    "            \n",
    "    tp = (y_true * y_pred).sum().float()\n",
    "    tn = ((1 - y_true) * (1 - y_pred)).sum().float()\n",
    "    fp = ((1 - y_true) * y_pred).sum().float()\n",
    "    fn = (y_true * (1 - y_pred)).sum().float()\n",
    "       \n",
    "    precision = tp / (tp + fp)\n",
    "    \n",
    "    return precision\n",
    "\n",
    "def recall(preds, y):\n",
    "    '''\n",
    "    Retourne le recall\n",
    "    '''\n",
    "    y_pred = torch.round(torch.sigmoid(preds))\n",
    "    y_true = (y_pred == y).float()       \n",
    "    \n",
    "    tp = (y_true * y_pred).sum().float()\n",
    "    tn = ((1 - y_true) * (1 - y_pred)).sum().float()\n",
    "    fp = ((1 - y_true) * y_pred).sum().float()\n",
    "    fn = (y_true * (1 - y_pred)).sum().float()\n",
    "    recall = tp / (tp + fn)\n",
    "    return recall\n",
    "\n",
    "\n",
    "def f1_loss(preds, y):\n",
    "    '''\n",
    "    Retourne le score F1\n",
    "    '''  \n",
    "    y_pred = torch.round(torch.sigmoid(preds))\n",
    "    y_true = (y_pred == y).float() \n",
    "            \n",
    "    tp = (y_true * y_pred).sum().float()\n",
    "    tn = ((1 - y_true) * (1 - y_pred)).sum().float()\n",
    "    fp = ((1 - y_true) * y_pred).sum().float()\n",
    "    fn = (y_true * (1 - y_pred)).sum().float()\n",
    "    \n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    \n",
    "    f1 = 2* (precision*recall) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cn8UE87Gteuc"
   },
   "source": [
    "La fonction `train` itère sur tous les exemples, un batch à la fois.\n",
    "\n",
    "`model.train()`est utilisé pour mettre le modèle en \"mode entraînement\", qui active _dropout_ et _batch normalization_.\n",
    "\n",
    "Pour chaque batch, on met à zéro le gradient. Chaque paramètre dans un modèle a un attribut `grad` qui stocke le gradient calculé par `criterion`. PyTorch ne met pas à zéro les gradients automatiquement du dernier calcul de graients. \n",
    "\n",
    "Nous introduisons ensuite le lot de phrases, `batch.text`, dans le modèle. Le `squeeze` est nécessaire car les prédictions sont initialement de taille _**[batch size, 1]**_, et nous devons supprimer la dimension de taille 1 car PyTorch s'attend à ce que les prédictions entrées dans notre fonction de critère soient de taille _**[batch size]**_.\n",
    "\n",
    "La perte et la précision sont ensuite calculées à l'aide de nos prédictions et des labels, `batch.label`, la perte étant moyennée sur tous les exemples du batch.\n",
    "\n",
    "Nous calculons le gradient de chaque paramètre avec `loss.backward ()`, puis mettons à jour les paramètres en utilisant les gradients et l'algorithme d'optimisation avec `optimizer.step ()`.\n",
    "\n",
    "La perte et la précision s'accumulent à travers l'epoch, la méthode `.item ()` est utilisée pour extraire un scalaire d'un tenseur qui ne contient qu'une seule valeur.\n",
    "\n",
    "Enfin, nous retournons la perte et la précision, moyennées sur toute l'époque. Le `len` d'un itérateur est le nombre de batchs dans l'itérateur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JUK2BZs6teud"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_prec = 0\n",
    "    epoch_rec = 0\n",
    "    epoch_f1 = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "               \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        prec = precision(predictions, batch.label)\n",
    "        rec = recall(predictions, batch.label)\n",
    "        f1 = f1_loss(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_prec += prec.item()\n",
    "        epoch_rec += rec.item()\n",
    "        epoch_f1 += f1.item()\n",
    "        \n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_prec / len(iterator), epoch_rec / len(iterator), epoch_f1 / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ufZOWJenteug"
   },
   "source": [
    "La fonction `evaluate` est similaire à` train`, avec quelques modifications car vous ne voulez pas mettre à jour les paramètres lors de l'évaluation.\n",
    "\n",
    "`model.eval ()` met le modèle en \"mode d'évaluation\", ceci désactive _dropout_ et _batch normalization_.\n",
    "\n",
    "Aucun gradient n'est calculé sur les opérations PyTorch à l'intérieur du bloc `with no_grad ()`. Cela réduit l'utilisation de la mémoire et accélère le calcul.\n",
    "\n",
    "Le reste de la fonction est identique à `train`, avec la suppression de` optimizer.zero_grad () `,` loss.backward () `et` optimizer.step () `, car nous ne mettons pas à jour les paramètres du modèle lorsque évaluer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K6B9sga2teuh"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model.forward(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FUF-j3L5teup"
   },
   "source": [
    "Nous entraînons ensuite le modèle à travers plusieurs époques, une époque étant un passage complet à travers tous les exemples dans les ensembles d'apprentissage et de validation.\n",
    "\n",
    "À chaque époque, si la perte de validation est la meilleure que nous ayons vue jusqu'à présent, nous enregistrerons les paramètres du modèle, puis une fois l'entraînement terminé, nous utiliserons ce modèle sur l'ensemble de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "colab_type": "code",
    "id": "k1oHzxVZteuq",
    "outputId": "b2da54ea-d1b2-40ce-dc9b-77be191b5239"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 506.00 MiB (GPU 0; 15.90 GiB total capacity; 575.19 MiB already allocated; 215.56 MiB free; 900.00 MiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-0989209a2efd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_prec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_rec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-8d28c54238d9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 506.00 MiB (GPU 0; 15.90 GiB total capacity; 575.19 MiB already allocated; 215.56 MiB free; 900.00 MiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "N_EPOCHS = 2\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc, train_prec, train_rec, train_f1 = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%| Train Precision: {train_prec*100:.2f}%| Train Recall: {train_rec*100:.2f}%| Train F1: {train_f1*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ysk_ln28teut"
   },
   "source": [
    "## Tester le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aTbFhWLrteut"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre fonction predict_sentiment fait plusieurs choses:\n",
    "\n",
    " - définit le modèle en mode d'évaluation\n",
    " - tokenise la phrase, c'est-à-dire la divise d'une chaîne brute en une liste de jetons\n",
    " - indexe les jetons en les convertissant en leur représentation entière à partir de notre vocabulaire\n",
    " - obtient la longueur de notre séquence\n",
    " - convertit les index, qui sont une liste Python en un tenseur PyTorch\n",
    " - ajouter une dimension de lot en relâchant\n",
    " - convertit la longueur en un tenseur\n",
    " - écrase la prédiction de sortie d'un nombre réel compris entre 0 et 1 avec la fonction sigmoïde\n",
    " - convertit le tenseur contenant une valeur unique en un entier avec la méthode item ()\n",
    "\n",
    "Nous nous attendons à ce que les avis avec un sentiment négatif renvoient une valeur proche de 0 et les avis positifs renvoient une valeur proche de 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    print(tokenized)\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    print(indexed)\n",
    "    length = [len(indexed)]\n",
    "    print(length)\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    print(tensor)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    print(tensor)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    print(length_tensor)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    print(prediction)\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentiment(model, \"This film is terrible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentiment(model, \"This film is great\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copie de 1 - Simple Sentiment Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
