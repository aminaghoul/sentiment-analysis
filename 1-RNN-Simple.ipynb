{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F4YxwviWtesV"
   },
   "source": [
    "# RNN pour l'analyse de sentiments\n",
    "\n",
    "Nous allons maintenant faire une analyse de sentiments sur le même jeu de données en utilisant les réseaux de neurones récurrents.\n",
    "\n",
    "## Réseaux de neurones récurrents\n",
    "\n",
    "Les réseaux de neurones récurrents ou (RNN), sont souvent utilisés pour analyser des séquences.\n",
    "En effet, dans les réseaux de neurones généraux, un input est traité par un certain nombre de couches et un output est produit à la sortie, avec l'hypothèse que deux inputs successifs sont indépendants.\n",
    "Cependant, cette hypothèse n'est pas correcte dans un certain nombre de scénarios. \n",
    "Par exemple, si on veut prédire le mot suivant dans une séquence, il est indispensable de considérer la dépendance des observations précédentes.\n",
    "    \n",
    "Dans notre cas, le modèle RNN prend une séquence de mots $X=\\{x_1, ..., x_T\\}$, une à la fois, et produit un état caché $h$, pour chaque mot.\n",
    "On utilise le RNN en lui donnant le mot courant $x_t$ ainsi que l'état caché du mot précédent, $h_{t-1}$, pour produire l'état caché suivant, $h_t$.\n",
    "\n",
    "\n",
    "$$h_t = \\text{RNN}(x_t, h_{t-1})$$\n",
    "\n",
    "\n",
    "Une fois que l'on a notre état caché final, $h_T$, obtenu après avoir donné le dernier mot de la séquence $x_T$ au modèle, on le donne à une couche linéaire $f$, (qui s'appelle également fully connected layer), pour recevoir notre sentiment prédit, $\\hat{y} = f(h_T)$.\n",
    "\n",
    "<center> <img src=\"RNN.png\" alt=\"drawing\" width=\"700\"/>\n",
    "        \n",
    "        \n",
    "Cette illustration montre un exemple de phrase, avec le RNN prédisant 0, c'est-à-dire que le sentiment est négatif. Le RNN est représenté en orange et la couche linéaire est en gris. On utilise le même RNN pour chaque mot, c'est-à-dire qu'il a les mêmes paramètres. L'état initial caché $h_0$, est un tensor initialisé à zéro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IINy-8uQtesY"
   },
   "source": [
    "## Préparation des données\n",
    "\n",
    "Dans ce notebook, on utilise la librairie torch et TorchText.\n",
    "\n",
    "TorchText a une méthode `Field` qui sert à définir comment les données brutes doivent être traitées.\n",
    "\n",
    "La méthode `TEXT` définit comment les commentaires doivent être traités, et `LABEL` comment les labels doivent être traités. \n",
    "Ces méthodes comportent plusieurs paramètres qui sont décrits dans la \n",
    "<a href=\"https://torchtext.readthedocs.io/en/latest/data.html#field\">documentation</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SV06LPyQtesa"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "# pour la reproductibilité\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(sequential=True,lower=True, tokenize = 'spacy')\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On sépare les données en données de train, de test et de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille des données train: 18163\n",
      "Taille des données de validation: 2270\n",
      "Taille des données test: 2271\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
    "        path='./data/', train='train.csv',\n",
    "        validation='valid.csv', test='test.csv', format='csv', skip_header=True,\n",
    "        fields=[('text', TEXT), ('label', LABEL)])\n",
    "\n",
    "print(f'Taille des données train: {len(train_data)}')\n",
    "print(f'Taille des données de validation: {len(valid_data)}')\n",
    "print(f'Taille des données test: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NOCX0qGNtesw"
   },
   "source": [
    "On affiche un exemple d'un commentaire \"tokénisé\" avec le label correspondant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "XXxXnQwctesx",
    "outputId": "bca3747b-655d-4ae7-fb2f-29b49a373eb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['i', 'really', 'wonder', 'how', 'this', 'show', 'plays', 'in', 'the', 'u.k.', 'and', 'the', 'rest', 'of', 'europe', '.', 'it', 'is', 'so', 'self', '-', 'loathing', 'about', 'being', 'an', 'american(particularly', 'white', 'americans', ')', '.', 'that', 'could', 'be', 'a', 'big', 'reason', 'for', 'some', 'of', 'the', 'venom', 'and', 'vitriol', 'expressed', 'here', 'on', 'this', 'board', '.', 'i', 'love', 'the', 'show', 'but', 'it', 'is', 'with', 'some', 'reservations', 'and', 'i', 'feel', 'it', 'took', 'the', 'easy', 'way', 'out', 'by', 'spoiler', 'spoiler:<br', '/><br', '/>crashing', 'and', 'burning', 'everything', 'at', 'the', 'end', 'of', 'the', 'second', 'season', '.', 'julie', 'slammed', 'the', 'door', 'shut', 'on', 'any', 'hope', 'of', 'reviving', 'the', 'show', 'unless', 'her', 'character', 'lands', 'on', 'a', 'haystack', 'in', 'the', 'middle', 'of', 'some', 'farm', '.', 'who', 'knows', '?', 'it', 'was', 'a', 'funny', ',', 'raunchy', 'and', 'on', 'occasion', ',', 'kinda', 'scary', 'show', '.'], 'label': '0'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de tokens unique dans le TEXT: 89987\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_data)\n",
    "print(f\"Nombre de tokens unique dans le TEXT: {len(TEXT.vocab)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2JwQ2BY0tetK"
   },
   "source": [
    "Le vocabulaire est composé de 89987 mots. On le réduit en ne prennant que 25000 mots pour ne pas que la dimension de la matrice d'embedding soit trop grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j8gQvkW7tetL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de tokens unique dans le TEXT: 25002\n",
      "Nombre unique de LABEL: 2\n"
     ]
    }
   ],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "print(f\"Nombre de tokens unique dans le TEXT: {len(TEXT.vocab)}\") \n",
    "print(f\"Nombre unique de LABEL: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H-EuaMfGtetU"
   },
   "source": [
    "Les deux valeurs supplémentaires dans le vocabulaire du TEXT sont les tokens `<unk>` et `<pad>`. Si un mot de l'échantillon de validation ou de test n'est pas dans le vocabulaire, alors celui-ci est remplacé par `<unk>`. Le token `<pad>` est utilisé pour que les séquences d'un même batch soient de mêmes longueurs. \n",
    "\n",
    "On affiche ci-dessous les 20 mots les plus fréquents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "m0yMEfHptetW",
    "outputId": "b4503fa3-92e7-479b-c64b-79797e8ad0db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 274223), (',', 239861), ('.', 204258), ('and', 135511), ('a', 135466), ('of', 121892), ('to', 112826), ('is', 92798), ('in', 77926), ('it', 74198), ('i', 60943), ('that', 60269), ('this', 55592), ('\"', 54672), (\"'s\", 54540), ('-', 45599), ('\\n', 42455), ('as', 39521), ('with', 37130), ('was', 36791)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nXX3_jXLtetn"
   },
   "source": [
    "Les valeurs du vocabulaire de LABEL sont 0 pour positif et 1 pour négatif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "P4i4-Fhjteto",
    "outputId": "9e71e222-9875-436f-e807-2d231626177a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'0': 0, '1': 1})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j8ovGiuJtett"
   },
   "source": [
    "La dernière étape de la préparation des données consiste à créer les itérateurs. Nous les parcourons dans la boucle d'apprentissage / d'évaluation, et ils retournent un batch à chaque itération.\n",
    "\n",
    "On utilise `BucketIterator` qui est un itérateur qui renverra un batch de séquences de mêmes longueurs grâce au padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vx_Fb_59tetu"
   },
   "outputs": [],
   "source": [
    "# utilisation du GPU si possible \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device, sort = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tz0537L1tetx"
   },
   "source": [
    "## Construction du modèle\n",
    "\n",
    "Le modèle RNN utilisé se compose des couches suivantes : \n",
    "\n",
    "- _embedding_ : utilisé pour transformer notre vecteur one hot encoder (dont la plupart des éléments sont des 0) en un vecteur embedding dense (dense car la dimensionnalité est beaucoup plus petite et tous les éléments sont des nombres réels). De plus, les mots qui ont un impact similaire sur le sentiment de la revue sont mappés de manière rapprochée dans cet espace vectoriel dense.\n",
    "- _RNN_ : prend le vecteur dense et le précédent état caché $h_{t-1}$, et calcule l'état caché suivant $h_t$.\n",
    "- _linéaire_ : prend le dernier état caché, le met dans un couche fully connected $f(h_T)$ qui le transfome en output prédit.\n",
    "\n",
    "\n",
    "Par ailleurs, le modèle considéré est bidirectionnel, il peut être représenté comme ceci.\n",
    "\n",
    "\n",
    "<center> <img src=\"birnn.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "\n",
    "En plus d'avoir un RNN traitant les mots de la phrase du premier au dernier (un RNN forward), nous avons un deuxième RNN traitant les mots de la phrase du dernier au premier (un RNN backward). Au pas de temps $t$, le RNN forward traite le mot $ x_t $, et le RNN backward traite le mot $ x_ {T-t + 1}\n",
    "\n",
    "Chaque batch, `text` est un tensor de dimension _**[taille séquence, taille batch]**_. C'est un batch de commentaires, dont les mots sont convertis en représentation one-hot encoder. PyTorch stocke  un vecteur one-hot comme sa valeur d'index.\n",
    "\n",
    "Le batch de l'input est ensuite passé à travers la couche embedding pour obtenir une représentation de vecteur dense de nos commentaires.\n",
    "`embedded` est un tensor de taille _**[taille séquence, taille batch, dim embedding]**_.\n",
    "\n",
    "`embedded`est ensuite introduit dans la couche RNN. Dans certains frameworks, vous devez alimenter l'état caché initial, $h_0$, dans le RNN, cependant dans PyTorch, si aucun état caché initial n'est passé comme argument, il prend par défaut un tenseur de valeurs zéros.\n",
    "\n",
    "The RNN retourne 2 tenseurs, `output` de taille _**[taille séquence, taille batch, dim état caché*2]**_ et `hidden` de taille _**[num direction, taille batch, dim état caché]**_.\n",
    "\n",
    "`output` est la concaténation de l'état caché de chaque pas de temps. Comme nous voulons les états cachés avant et arrière de la couche finale (supérieure), nous obtenons les deux couches cachées supérieures de la première dimension, hidden [-2,:,:] et hidden [-1,:,:], et les concaténons ensemble ( _**[taille batch, dim état caché*2]**_ ) avant de les transmettre à la couche linéaire, pour produire une prédiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O8BPYwRGtety"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim,bidirectional):\n",
    "  \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim) \n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, bidirectional=bidirectional)   \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "\n",
    "        embedded = self.embedding(text) \n",
    "        output, hidden = self.rnn(embedded) \n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        \n",
    "        return self.fc(hidden)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1KBgfaFitet5"
   },
   "source": [
    "On crée alors notre modèle.\n",
    "\n",
    " - La dimension de l'input est la taille du vocabulaire. \n",
    " - La dimension de l'embedding est la taille des vecteurs denses. Elle est généralement de 50 à 250, mais elle dépend de la taille du vocabulaire.\n",
    " - La dimension de hidden est la dimension des états cachés. Elle vaut généralement 100 à 500, mais dépend de la taille du vocabulaire, des vecteurs denses et la compléxité de la tâche. \n",
    " - La dimension de l'output est le nombre de classes, ici, la valeur de l'output est entre 0 et 1 et donc est de dimension 1 car l'output est un scalaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3A_TUXSftet8"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "BIDIRECTIONAL = True\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,BIDIRECTIONAL )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DHhyWBk3teuA",
    "outputId": "9586cff6-25a5-49cf-c85f-771a20ee6a29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le modèle a 2,684,009 paramètres à entraîner\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'Le modèle a {count_parameters(model):,} paramètres à entraîner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zGjJ1BSSteuF"
   },
   "source": [
    "## Entraîner le modèle\n",
    "\n",
    "On utilise l'optimiseur Adam qui ne prend en compte seulement comme paramètres les paramètres du modèle qui seront actualisés par l'optimiseur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MC34zFI4teuI"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ldK_vHILteuN"
   },
   "source": [
    "On définit ensuite la fonction de perte qui est ici la _binary cross entropy with logits_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bEn4T95ZteuN"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NpprToLYteuQ"
   },
   "source": [
    "En utilisant `.to`, nous pouvons placer le modèle et le critère sur le GPU (si nous en avons un)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kh294K1JteuR"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w42XP6EbteuV"
   },
   "source": [
    "Notre fonction criterion calcule la perte, cependant nous devons écrire notre fonction pour calculer l'accuracy, le recall et le f1score.\n",
    "\n",
    " - Accuracy : \n",
    "$$accuracy = \\frac{TP+TN}{TP+TN+FP+FN}\\,,$$\n",
    "\n",
    " - Recall :\n",
    "$$recall = \\frac{TP}{TP+FN}\\,,$$\n",
    "\n",
    " - F1 score : \n",
    "$$f1score = 2*\\frac{precision*recall}{precision+recall}\\,,$$\n",
    " \n",
    "avec $TP$ : vrais positifs, $TN$ : vrais négatifs, $FP$ : faux positifs, $FN$ : faux négatifs et $precision = \\frac{TP}{TP+FP}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4G8cW2IpteuX"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Retourne l'accuracy par batch\n",
    "    \"\"\"\n",
    "    #arrondi la prédiction à l'entier le plus proche\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def recall(preds, y):\n",
    "    '''\n",
    "    Retourne le recall\n",
    "    '''\n",
    "    y_pred = torch.round(torch.sigmoid(preds))\n",
    "    y_true = (y_pred == y).float()       \n",
    "    \n",
    "    tp = (y_true * y_pred).sum().float()\n",
    "    tn = ((1 - y_true) * (1 - y_pred)).sum().float()\n",
    "    fp = ((1 - y_true) * y_pred).sum().float()\n",
    "    fn = (y_true * (1 - y_pred)).sum().float()\n",
    "    \n",
    "    if (tp + fn) == 0:\n",
    "        recall = torch.zeros(1)\n",
    "        \n",
    "    recall = tp / (tp + fn)\n",
    "    return recall\n",
    "\n",
    "\n",
    "\n",
    "def f1_loss(preds, y):\n",
    "    '''\n",
    "    Retourne le score F1\n",
    "    '''  \n",
    "    y_pred = torch.round(torch.sigmoid(preds))\n",
    "    y_true = (y_pred == y).float() \n",
    "            \n",
    "    tp = (y_true * y_pred).sum().float()\n",
    "    tn = ((1 - y_true) * (1 - y_pred)).sum().float()\n",
    "    fp = ((1 - y_true) * y_pred).sum().float()\n",
    "    fn = (y_true * (1 - y_pred)).sum().float()\n",
    "    \n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    \n",
    "    if (tp + fn) == 0 or (tp + fp) == 0 or (recall + precision == 0):\n",
    "        f1 = torch.zeros(1)\n",
    "    else:\n",
    "        f1 = 2* (precision*recall) / (precision + recall)\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cn8UE87Gteuc"
   },
   "source": [
    "La fonction `train` itère sur tous les exemples, un batch à la fois.\n",
    "\n",
    "`model.train()`est utilisé pour mettre le modèle en \"mode entraînement\", qui active _dropout_ et _batch normalization_, qui ne sont pas utilisés ici.\n",
    "\n",
    "Pour chaque batch, on met à zéro le gradient. Chaque paramètre dans un modèle a un attribut `grad` qui stocke le gradient calculé par `criterion`. PyTorch ne met pas à zéro les gradients automatiquement.\n",
    "\n",
    "Nous introduisons ensuite le batch de phrases, `batch.text`, dans le modèle. Le `squeeze` est nécessaire car les prédictions sont initialement de taille _**[taille batch, 1]**_, et nous devons supprimer la dimension de taille 1 car PyTorch s'attend à ce que les prédictions entrées dans notre fonction de critère soient de taille _**[taille batch]**_.\n",
    "La perte et la précision sont ensuite calculées à l'aide de nos prédictions et des labels, `batch.label`, la perte étant moyennée sur toutes les séquences du batch.\n",
    "\n",
    "Nous calculons le gradient de chaque paramètre avec `loss.backward ()`, puis mettons à jour les paramètres en utilisant les gradients et l'algorithme d'optimisation avec `optimizer.step ()`.\n",
    "\n",
    "Enfin, nous retournons la perte, la précision, le recall et le F1 score moyennés sur toute l'époque. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JUK2BZs6teud"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_rec = 0\n",
    "    epoch_f1 = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "               \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        rec = recall(predictions, batch.label)\n",
    "        f1 = f1_loss(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_rec += rec.item()\n",
    "        epoch_f1 += f1.item()        \n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_rec / len(iterator), epoch_f1 / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ufZOWJenteug"
   },
   "source": [
    "La fonction `evaluate` est similaire à` train`, avec quelques modifications car vous ne voulez pas mettre à jour les paramètres lors de l'évaluation.\n",
    "\n",
    "`model.eval ()` met le modèle en \"mode d'évaluation\", ceci désactive _dropout_ et _batch normalization_.\n",
    "\n",
    "Aucun gradient n'est calculé sur les opérations PyTorch à l'intérieur du bloc `with no_grad ()`. Cela réduit l'utilisation de la mémoire et accélère le calcul.\n",
    "\n",
    "Le reste de la fonction est identique à `train`, avec la suppression de` optimizer.zero_grad () `,` loss.backward () `et` optimizer.step () `, car nous ne mettons pas à jour les paramètres du modèle lors de l'évaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K6B9sga2teuh"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_rec = 0\n",
    "    epoch_f1 = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model.forward(batch.text).squeeze(1) \n",
    "            loss = criterion(predictions, batch.label)    \n",
    "\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "            rec = recall(predictions, batch.label)\n",
    "            f1 = f1_loss(predictions, batch.label)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_rec += rec.item()\n",
    "            epoch_f1 += f1.item() \n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_rec / len(iterator), epoch_f1 / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FUF-j3L5teup"
   },
   "source": [
    "Nous entraînons ensuite le modèle sur plusieurs époques, une époque étant un passage complet à travers toutes les séquences dans les ensembles d'apprentissage et de validation.\n",
    "\n",
    "À chaque époque, si la perte de validation est la meilleure que nous ayons vue jusqu'à présent, nous enregistrerons les paramètres du modèle, puis une fois l'entraînement terminé, nous utiliserons ce modèle sur l'ensemble de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "colab_type": "code",
    "id": "k1oHzxVZteuq",
    "outputId": "b2da54ea-d1b2-40ce-dc9b-77be191b5239"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 1m 7s\n",
      "\tTrain Loss: 0.662 | Train Acc: 59.96% | Train Recall: 34.19% | Train F1: 40.36%\n",
      "\t Val. Loss: 0.680 |  Val. Acc: 57.82% | Val. Recall: 37.87%  | Val. F1: 44.33%\n",
      "Epoch: 02 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.640 | Train Acc: 63.44% | Train Recall: 38.76% | Train F1: 45.87%\n",
      "\t Val. Loss: 0.659 |  Val. Acc: 61.11% | Val. Recall: 43.71%  | Val. F1: 49.65%\n",
      "Epoch: 03 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.605 | Train Acc: 67.98% | Train Recall: 41.23% | Train F1: 49.87%\n",
      "\t Val. Loss: 0.645 |  Val. Acc: 63.83% | Val. Recall: 43.32%  | Val. F1: 50.61%\n",
      "Epoch: 04 | Epoch Time: 1m 13s\n",
      "\tTrain Loss: 0.588 | Train Acc: 69.57% | Train Recall: 39.65% | Train F1: 49.28%\n",
      "\t Val. Loss: 0.744 |  Val. Acc: 56.00% | Val. Recall: 43.65%  | Val. F1: 40.93%\n",
      "Epoch: 05 | Epoch Time: 1m 7s\n",
      "\tTrain Loss: 0.576 | Train Acc: 70.07% | Train Recall: 40.28% | Train F1: 49.52%\n",
      "\t Val. Loss: 0.648 |  Val. Acc: 64.30% | Val. Recall: 45.37%  | Val. F1: 51.97%\n",
      "Epoch: 06 | Epoch Time: 1m 17s\n",
      "\tTrain Loss: 0.518 | Train Acc: 75.70% | Train Recall: 43.24% | Train F1: 54.07%\n",
      "\t Val. Loss: 0.649 |  Val. Acc: 66.55% | Val. Recall: 44.70%  | Val. F1: 52.49%\n",
      "Epoch: 07 | Epoch Time: 1m 5s\n",
      "\tTrain Loss: 0.483 | Train Acc: 78.36% | Train Recall: 42.79% | Train F1: 54.68%\n",
      "\t Val. Loss: 0.655 |  Val. Acc: 65.91% | Val. Recall: 37.74%  | Val. F1: 47.81%\n",
      "Epoch: 08 | Epoch Time: 1m 15s\n",
      "\tTrain Loss: 0.475 | Train Acc: 79.10% | Train Recall: 43.11% | Train F1: 55.11%\n",
      "\t Val. Loss: 0.680 |  Val. Acc: 66.25% | Val. Recall: 42.33%  | Val. F1: 50.92%\n",
      "Epoch: 09 | Epoch Time: 1m 9s\n",
      "\tTrain Loss: 0.445 | Train Acc: 80.99% | Train Recall: 43.62% | Train F1: 56.05%\n",
      "\t Val. Loss: 0.642 |  Val. Acc: 68.30% | Val. Recall: 41.13%  | Val. F1: 51.26%\n",
      "Epoch: 10 | Epoch Time: 1m 7s\n",
      "\tTrain Loss: 0.422 | Train Acc: 82.48% | Train Recall: 43.87% | Train F1: 56.66%\n",
      "\t Val. Loss: 0.653 |  Val. Acc: 67.20% | Val. Recall: 41.38%  | Val. F1: 50.67%\n",
      "Epoch: 11 | Epoch Time: 1m 15s\n",
      "\tTrain Loss: 0.397 | Train Acc: 84.04% | Train Recall: 43.42% | Train F1: 56.87%\n",
      "\t Val. Loss: 0.668 |  Val. Acc: 67.53% | Val. Recall: 45.71%  | Val. F1: 53.52%\n",
      "Epoch: 12 | Epoch Time: 1m 6s\n",
      "\tTrain Loss: 0.407 | Train Acc: 83.35% | Train Recall: 43.05% | Train F1: 56.30%\n",
      "\t Val. Loss: 0.690 |  Val. Acc: 69.86% | Val. Recall: 44.57%  | Val. F1: 53.73%\n",
      "Epoch: 13 | Epoch Time: 1m 15s\n",
      "\tTrain Loss: 0.386 | Train Acc: 84.67% | Train Recall: 43.85% | Train F1: 57.24%\n",
      "\t Val. Loss: 0.673 |  Val. Acc: 69.56% | Val. Recall: 42.91%  | Val. F1: 52.71%\n",
      "Epoch: 14 | Epoch Time: 1m 9s\n",
      "\tTrain Loss: 0.357 | Train Acc: 86.12% | Train Recall: 43.46% | Train F1: 57.49%\n",
      "\t Val. Loss: 0.709 |  Val. Acc: 68.55% | Val. Recall: 45.07%  | Val. F1: 53.68%\n",
      "Epoch: 15 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.347 | Train Acc: 86.56% | Train Recall: 43.96% | Train F1: 57.89%\n",
      "\t Val. Loss: 0.689 |  Val. Acc: 70.74% | Val. Recall: 46.42%  | Val. F1: 55.12%\n",
      "Epoch: 16 | Epoch Time: 1m 14s\n",
      "\tTrain Loss: 0.348 | Train Acc: 86.48% | Train Recall: 43.88% | Train F1: 57.76%\n",
      "\t Val. Loss: 0.658 |  Val. Acc: 71.06% | Val. Recall: 43.77%  | Val. F1: 53.76%\n",
      "Epoch: 17 | Epoch Time: 1m 7s\n",
      "\tTrain Loss: 0.308 | Train Acc: 88.65% | Train Recall: 43.97% | Train F1: 58.44%\n",
      "\t Val. Loss: 0.683 |  Val. Acc: 70.53% | Val. Recall: 46.76%  | Val. F1: 55.18%\n",
      "Epoch: 18 | Epoch Time: 1m 16s\n",
      "\tTrain Loss: 0.303 | Train Acc: 88.88% | Train Recall: 44.62% | Train F1: 58.89%\n",
      "\t Val. Loss: 0.677 |  Val. Acc: 68.87% | Val. Recall: 41.33%  | Val. F1: 51.55%\n",
      "Epoch: 19 | Epoch Time: 1m 7s\n",
      "\tTrain Loss: 0.345 | Train Acc: 86.31% | Train Recall: 44.44% | Train F1: 58.02%\n",
      "\t Val. Loss: 0.691 |  Val. Acc: 70.40% | Val. Recall: 42.57%  | Val. F1: 52.90%\n",
      "Epoch: 20 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.317 | Train Acc: 88.11% | Train Recall: 44.32% | Train F1: 58.49%\n",
      "\t Val. Loss: 0.700 |  Val. Acc: 70.76% | Val. Recall: 44.21%  | Val. F1: 53.91%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc, train_rec, train_f1 = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc, valid_rec, valid_f1 = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Train Recall: {train_rec*100:.2f}% | Train F1: {train_f1*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% | Val. Recall: {valid_rec*100:.2f}%  | Val. F1: {valid_f1*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ysk_ln28teut"
   },
   "source": [
    "## Tester le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aTbFhWLrteut"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.648 | Test Acc: 68.21%| Test Recall: 40.61%  | Test F1: 50.41%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "\n",
    "test_loss, test_acc, test_rec, test_f1 = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%| Test Recall: {test_rec*100:.2f}%  | Test F1: {test_f1*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inférence\n",
    "\n",
    "Notre fonction predict_sentiment fait plusieurs choses:\n",
    "\n",
    " - définit le modèle en mode d'évaluation\n",
    " - tokenise la phrase, c'est-à-dire la divise d'une chaîne brute en une liste de jetons\n",
    " - indexe les jetons en les convertissant en leur représentation entière à partir de notre vocabulaire\n",
    " - obtient la longueur de notre séquence\n",
    " - convertit les index, qui sont une liste Python en un tenseur PyTorch\n",
    " - ajouter une dimension de lot en relâchant\n",
    " - convertit la longueur en un tenseur\n",
    " - écrase la prédiction de sortie d'un nombre réel compris entre 0 et 1 avec la fonction sigmoïde\n",
    " - convertit le tenseur contenant une valeur unique en un entier avec la méthode item ()\n",
    "\n",
    "Nous nous attendons à ce que les avis avec un sentiment négatif renvoient une valeur proche de 0 et les avis positifs renvoient une valeur proche de 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9002652764320374"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"This film is terrible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07549090683460236"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"This film is great\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que ce modèle nous donne de faible résultats. En utilisant un modèle plus avancé on obtiendrait de meilleurs résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réference : \n",
    "\n",
    " - https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copie de 1 - Simple Sentiment Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
