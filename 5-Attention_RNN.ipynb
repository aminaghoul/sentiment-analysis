{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM et attention\n",
    "\n",
    "Nous allons dans ce notebook utiliser le modèle LSTM combiné avec le modèle d'attention, qui est expliqué dans cet <a href=\"https://arxiv.org/pdf/1706.03762.pdf\"> article</a>.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.train_iterator = None\n",
    "        self.test_iterator = None\n",
    "        self.val_iterator = None\n",
    "        self.vocab = []\n",
    "        self.word_embeddings = {}\n",
    "    \n",
    "        '''\n",
    "        Loads the data from files\n",
    "        Sets up iterators for training, validation and test data\n",
    "        Also create vocabulary and word embeddings based on the data\n",
    "        \n",
    "        Inputs:\n",
    "            w2v_file (String): path to file containing word embeddings (GloVe/Word2Vec)\n",
    "            train_file (String): path to training file\n",
    "            test_file (String): path to test file\n",
    "            val_file (String): path to validation file\n",
    "        '''\n",
    "\n",
    "       \n",
    "        # Creating Field for data\n",
    "\n",
    "        TEXT = data.Field(sequential=True,lower=True, tokenize = 'spacy', include_lengths=True)\n",
    "        LABEL = data.LabelField(sequential=False, use_vocab=False,dtype = torch.float)\n",
    "        train_data, valid_data, test_data = data.TabularDataset.splits(\n",
    "        path='./data/', train='train.csv',\n",
    "        validation='valid.csv', test='test.csv', format='csv', skip_header=True,\n",
    "        fields=[('text', TEXT), ('label', LABEL)])\n",
    "        \n",
    "\n",
    "        TEXT.build_vocab(train_data, \n",
    "                 max_size = self.config.max_vocab_size, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "        \n",
    "        LABEL.build_vocab(train_data)\n",
    "        print(LABEL.vocab.stoi)\n",
    "        self.word_embeddings = TEXT.vocab.vectors\n",
    "        self.vocab = TEXT.vocab\n",
    "        self.PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "        self.UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.train_iterator= data.BucketIterator(\n",
    "            (train_data),\n",
    "            batch_size= self.config.batch_size,\n",
    "            device = device,\n",
    "            sort_key=lambda x: len(x.text),\n",
    "            sort_within_batch = True)\n",
    "        \n",
    "        self.val_iterator, self.test_iterator = data.BucketIterator.splits(\n",
    "            (valid_data, test_data),\n",
    "            batch_size=self.config.batch_size,\n",
    "            device = device,\n",
    "            sort_key=lambda x: len(x.text),\n",
    "            sort_within_batch = True)\n",
    "        \n",
    "        print(f'Taille des données train: {len(train_data)}')\n",
    "        print(f'Taille des données de validation: {len(valid_data)}')\n",
    "        print(f'Taille des données test: {len(test_data)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construire le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    max_vocab_size = 50000\n",
    "    embed_size = 100\n",
    "    hidden_layers = 1\n",
    "    hidden_size = 256\n",
    "    bidirectional = True\n",
    "    output_size = 1\n",
    "    max_epochs = 15\n",
    "    batch_size = 64\n",
    "    dropout_keep = 0.6\n",
    "    max_sen_len = None # Sequence length for RNN    \n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'1': 0, '0': 1})\n",
      "Taille des données train: 18163\n",
      "Taille des données de validation: 2270\n",
      "Taille des données test: 2271\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Seq2SeqAttention(nn.Module):\n",
    "    def __init__(self, config, vocab_size, pretrained_embeddings):\n",
    "        super(Seq2SeqAttention, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Embedding Layer\n",
    "        self.embeddings = nn.Embedding(vocab_size, self.config.embed_size)\n",
    "\n",
    "        #self.embeddings.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "        #UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "        #self.embeddings.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "        #self.embeddings.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "        #self.embeddings.weight = nn.Parameter(pretrained_embeddings, requires_grad=False)\n",
    "        \n",
    "        # Encoder RNN\n",
    "        self.rnn = nn.LSTM(input_size = self.config.embed_size,\n",
    "                            hidden_size = self.config.hidden_size,\n",
    "                            num_layers = self.config.hidden_layers,\n",
    "                            bidirectional = self.config.bidirectional)\n",
    "        \n",
    "        # Dropout Layer\n",
    "        self.dropout = nn.Dropout(self.config.dropout_keep)\n",
    "        \n",
    "        # Fully-Connected Layer\n",
    "        self.fc = nn.Linear(\n",
    "            self.config.hidden_size * 2 * (1+self.config.bidirectional) \n",
    "            ,\n",
    "            self.config.output_size\n",
    "        )\n",
    "        \n",
    "        # Softmax non-linearity\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "                \n",
    "    def apply_attention(self, rnn_output, final_hidden_state):\n",
    "        '''\n",
    "        Apply Attention on RNN output\n",
    "        \n",
    "        Input:\n",
    "            rnn_output (batch_size, seq_len, num_directions * hidden_size): tensor representing hidden state for every word in the sentence\n",
    "            final_hidden_state (batch_size, num_directions * hidden_size): final hidden state of the RNN\n",
    "            \n",
    "        Returns:\n",
    "            attention_output(batch_size, num_directions * hidden_size): attention output vector for the batch\n",
    "        '''\n",
    "        hidden_state = final_hidden_state.unsqueeze(2)\n",
    "        attention_scores = torch.bmm(rnn_output, hidden_state).squeeze(2)\n",
    "        soft_attention_weights = F.softmax(attention_scores, 1).unsqueeze(2) #shape = (batch_size, seq_len, 1)\n",
    "        attention_output = torch.bmm(rnn_output.permute(0,2,1), soft_attention_weights).squeeze(2)\n",
    "        return attention_output\n",
    "        \n",
    "    def forward(self, x, text_lengths):\n",
    "        # x.shape = (max_sen_len, batch_size)\n",
    "        embedded_sent = self.embeddings(x)\n",
    "        # embedded_sent.shape = (max_sen_len=20, batch_size=64,embed_size=300)\n",
    "\n",
    "        ##################################### Encoder #######################################\n",
    "        #lstm_output, (h_n,c_n) = self.lstm(embedded_sent)\n",
    "        # lstm_output.shape = (seq_len, batch_size, num_directions * hidden_size)\n",
    "         # embedded_sent.shape = (max_sen_len=20, batch_size=64,embed_size=300)\n",
    "        #pack sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded_sent, text_lengths)\n",
    "        packed_output, (h_n,c_n) = self.rnn(packed_embedded)    \n",
    "        #unpack sequence\n",
    "        lstm_output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        \n",
    "        ##################################### Encoder ####################################### \n",
    "        # lstm_output.shape = (seq_len, batch_size, num_directions * hidden_size)\n",
    "        \n",
    "        # Final hidden state of last layer (num_directions, batch_size, hidden_size)\n",
    "        # Final hidden state of last layer (num_directions, batch_size, hidden_size)\n",
    "        batch_size = h_n.shape[1]\n",
    "        h_n_final_layer = h_n.view(self.config.hidden_layers,\n",
    "                                   self.config.bidirectional + 1,\n",
    "                                   batch_size,\n",
    "                                   self.config.hidden_size)[-1,:,:,:]\n",
    "        \n",
    "        ##################################### Attention #####################################\n",
    "        # Convert input to (batch_size, num_directions * hidden_size) for attention\n",
    "        final_hidden_state = torch.cat([h_n_final_layer[i,:,:] for i in range(h_n_final_layer.shape[0])], dim=1)\n",
    "        \n",
    "        attention_out = self.apply_attention(lstm_output.permute(1,0,2), final_hidden_state)\n",
    "        # Attention_out.shape = (batch_size, num_directions * hidden_size)\n",
    "        \n",
    "        #################################### Linear #########################################\n",
    "        concatenated_vector = torch.cat([final_hidden_state, attention_out], dim=1)\n",
    "        final_feature_map = self.dropout(concatenated_vector) # shape=(batch_size, num_directions * hidden_size)\n",
    "        final_out = self.fc(final_feature_map)\n",
    "        return self.softmax(final_out)\n",
    "    \n",
    "    def add_optimizer(self, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def add_loss_op(self, loss_op):\n",
    "        self.loss_op = loss_op\n",
    " \n",
    "    def binary_accuracy(self, preds, y):\n",
    "        \"\"\"\n",
    "        Returns accuracy per batch\n",
    "        \"\"\"\n",
    "        #round predictions to the closest integer\n",
    "        rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "        correct = (rounded_preds == y).float() #convert into float for division \n",
    "        acc = correct.sum() / len(correct)\n",
    "        return acc \n",
    "    \n",
    "    def train_model(self, iterator):\n",
    "    \n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "    \n",
    "        self.train()\n",
    "    \n",
    "        for batch in iterator:\n",
    "        \n",
    "            self.optimizer.zero_grad()\n",
    "        \n",
    "            text, text_lengths = batch.text\n",
    "        \n",
    "            predictions = self.__call__(text, text_lengths).squeeze(1)\n",
    "\n",
    "            loss = self.loss_op(predictions, batch.label)\n",
    "            acc = self.binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "            loss.backward()\n",
    "        \n",
    "            optimizer.step()\n",
    "        \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "    \n",
    "    def evaluate_model(self, iterator):\n",
    "    \n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "    \n",
    "        self.eval()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "    \n",
    "            for batch in iterator:\n",
    "        \n",
    "                text, text_lengths = batch.text\n",
    "        \n",
    "                predictions = self.__call__(text, text_lengths).squeeze(1)\n",
    "\n",
    "                loss = self.loss_op(predictions, batch.label)\n",
    "            \n",
    "                acc = self.binary_accuracy(predictions, batch.label)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "        \n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)  \n",
    "    \n",
    "    def run_epoch(self, train_iterator, val_iterator, epoch):\n",
    "        \n",
    "        for i, batch in enumerate(train_iterator):\n",
    "    \n",
    "            if i % 100 == 0:\n",
    "                print(\"Iter: {}\".format(i+1))\n",
    "                train_loss, train_acc = self.train_model(train_iterator)\n",
    "                valid_loss, valid_acc = self.evaluate_model(val_iterator)\n",
    "                print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "                print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Iter: 1\n",
      "\tTrain Loss: 0.767 | Train Acc: 54.64%\n",
      "\t Val. Loss: 0.769 |  Val. Acc: 54.46%\n",
      "Epoch: 1\n",
      "Iter: 1\n",
      "\tTrain Loss: 0.767 | Train Acc: 54.63%\n",
      "\t Val. Loss: 0.769 |  Val. Acc: 54.46%\n",
      "Epoch: 2\n",
      "Iter: 1\n",
      "\tTrain Loss: 0.767 | Train Acc: 54.63%\n",
      "\t Val. Loss: 0.769 |  Val. Acc: 54.46%\n",
      "Epoch: 3\n",
      "Iter: 1\n",
      "\tTrain Loss: 0.767 | Train Acc: 54.63%\n",
      "\t Val. Loss: 0.769 |  Val. Acc: 54.46%\n",
      "Epoch: 4\n",
      "Iter: 1\n",
      "\tTrain Loss: 0.767 | Train Acc: 54.64%\n",
      "\t Val. Loss: 0.769 |  Val. Acc: 54.46%\n",
      "Epoch: 5\n",
      "Iter: 1\n",
      "\tTrain Loss: 0.767 | Train Acc: 54.64%\n",
      "\t Val. Loss: 0.769 |  Val. Acc: 54.46%\n",
      "Epoch: 6\n",
      "Iter: 1\n",
      "\tTrain Loss: 0.767 | Train Acc: 54.63%\n",
      "\t Val. Loss: 0.769 |  Val. Acc: 54.46%\n",
      "Epoch: 7\n",
      "Iter: 1\n",
      "\tTrain Loss: 0.767 | Train Acc: 54.64%\n",
      "\t Val. Loss: 0.769 |  Val. Acc: 54.46%\n",
      "Epoch: 8\n",
      "Iter: 1\n",
      "\tTrain Loss: 0.767 | Train Acc: 54.64%\n",
      "\t Val. Loss: 0.769 |  Val. Acc: 54.46%\n",
      "Epoch: 9\n",
      "Iter: 1\n",
      "\tTrain Loss: 0.767 | Train Acc: 54.64%\n",
      "\t Val. Loss: 0.769 |  Val. Acc: 54.46%\n",
      "Epoch: 10\n",
      "Iter: 1\n",
      "\tTrain Loss: 0.767 | Train Acc: 54.64%\n",
      "\t Val. Loss: 0.769 |  Val. Acc: 54.46%\n",
      "Epoch: 11\n",
      "Iter: 1\n",
      "\tTrain Loss: 0.767 | Train Acc: 54.63%\n",
      "\t Val. Loss: 0.769 |  Val. Acc: 54.46%\n",
      "Epoch: 12\n",
      "Iter: 1\n",
      "\tTrain Loss: 0.767 | Train Acc: 54.64%\n",
      "\t Val. Loss: 0.769 |  Val. Acc: 54.46%\n",
      "Epoch: 13\n",
      "Iter: 1\n",
      "\tTrain Loss: 0.767 | Train Acc: 54.64%\n",
      "\t Val. Loss: 0.769 |  Val. Acc: 54.46%\n",
      "Epoch: 14\n",
      "Iter: 1\n",
      "\tTrain Loss: 0.767 | Train Acc: 54.64%\n",
      "\t Val. Loss: 0.769 |  Val. Acc: 54.46%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext.vocab import Vectors\n",
    "import spacy\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "# Create Model with specified optimizer and loss function\n",
    "##############################################################\n",
    "\n",
    "model = Seq2SeqAttention(config, len(dataset.vocab), dataset.word_embeddings)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "pretrained_embeddings = dataset.vocab.vectors\n",
    "model.embeddings.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "model.embeddings.weight.data[dataset.UNK_IDX] = torch.zeros(config.embed_size)\n",
    "model.embeddings.weight.data[dataset.PAD_IDX] = torch.zeros(config.embed_size)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = criterion.to(device)\n",
    "model.add_optimizer(optimizer)\n",
    "model.add_loss_op(criterion)\n",
    "\n",
    "##############################################################\n",
    "\n",
    "for i in range(config.max_epochs):\n",
    "    print (\"Epoch: {}\".format(i))\n",
    "    model.run_epoch(dataset.train_iterator, dataset.val_iterator, i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 5,734,409 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(preds, y):\n",
    "    '''\n",
    "    Retourne le recall\n",
    "    '''\n",
    "    y_pred = torch.round(torch.sigmoid(preds))\n",
    "    y_true = (y_pred == y).float()       \n",
    "    \n",
    "    tp = (y_true * y_pred).sum().float()\n",
    "    tn = ((1 - y_true) * (1 - y_pred)).sum().float()\n",
    "    fp = ((1 - y_true) * y_pred).sum().float()\n",
    "    fn = (y_true * (1 - y_pred)).sum().float()\n",
    "    recall = tp / (tp + fn)\n",
    "    return recall\n",
    "\n",
    "\n",
    "def f1_loss(preds, y):\n",
    "    '''\n",
    "    Retourne le score F1\n",
    "    '''  \n",
    "    y_pred = torch.round(torch.sigmoid(preds))\n",
    "    y_true = (y_pred == y).float() \n",
    "            \n",
    "    tp = (y_true * y_pred).sum().float()\n",
    "    tn = ((1 - y_true) * (1 - y_pred)).sum().float()\n",
    "    fp = ((1 - y_true) * y_pred).sum().float()\n",
    "    fn = (y_true * (1 - y_pred)).sum().float()\n",
    "    \n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    \n",
    "    f1 = 2* (precision*recall) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.787 | Test Acc: 52.66%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate_model(dataset.test_iterator)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [dataset.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7310585975646973"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"This film is great and awesome as well it is incredible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7310585975646973"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"This film is terrible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
