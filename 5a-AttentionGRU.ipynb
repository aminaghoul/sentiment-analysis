{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "gtoDrTNFfyx1",
    "outputId": "b58108f0-766d-4782-ad22-aa4e5ceb2825"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ifdYYtURf2pB"
   },
   "outputs": [],
   "source": [
    "#my_path = \"drive/My Drive/data/\"\n",
    "my_path = \"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SOLYaAAQ1Ceu"
   },
   "source": [
    "# GRU + Attention\n",
    "\n",
    "Nous allons utiliser un modèle comprenant un réseau LSTM ainsi qu'une couche d'attention.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lm1iC_aQ13if"
   },
   "source": [
    "## Préparer les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "N7eAcLKmQWxa",
    "outputId": "e548134d-26bf-40cd-9382-aad14446256c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille des données train: 25000\n",
      "Taille des données de validation: 12500\n",
      "Taille des données test: 12500\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "# pour la reproductibilité\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(sequential=True,lower=True, tokenize = 'spacy', include_lengths=True, batch_first=True, fix_length=200)\n",
    "LABEL = data.LabelField(dtype = torch.float)\n",
    "\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "test_data, valid_data = torch.split(test_data, 0.5, dim=1) \n",
    "\n",
    "print(f'Taille des données train: {len(train_data)}')\n",
    "print(f'Taille des données de validation: {len(valid_data)}')\n",
    "print(f'Taille des données test: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de tokens unique dans le TEXT: 25002\n",
      "Nombre unique de LABEL: 2\n"
     ]
    }
   ],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE, vectors=GloVe(name='6B', dim=300))\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "print(f\"Nombre de tokens unique dans le TEXT: {len(TEXT.vocab)}\") \n",
    "print(f\"Nombre unique de LABEL: {len(LABEL.vocab)}\")\n",
    "\n",
    "\n",
    "# utilisation du GPU si possible \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "BATCH_SIZE = 32\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "device = device, sort_key=lambda x: len(x.text), repeat=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rUunWgd_1_eH"
   },
   "source": [
    "## Construire le modèle\n",
    "\n",
    "Nous allons incorporer le mécanisme Attention dans notre modèle LSTM. Dans ce nouveau modèle, nous utiliserons l'attention pour calculer le score \"soft\" d'alignement entre chacun des hidden_state et le dernier hidden_state du LSTM. \n",
    "\n",
    "Nous utiliserons torch.bmm pour la multiplication de la matrice par batch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class AttentionModel(nn.Module):\n",
    "    def __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings, dropout):\n",
    "        super(AttentionModel, self).__init__()\n",
    "\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_length = embedding_length\n",
    "        self.weights = word_embeddings\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
    "        self.word_embeddings.weights = nn.Parameter(word_embeddings, requires_grad=False)\n",
    "        self.dropout = 0.8\n",
    "        self.bilstm = nn.LSTM(embedding_length, hidden_size, dropout=self.dropout, bidirectional=True)\n",
    "        # We will use da = 350, r = 30 & penalization_coeff = 1 as per given in the self-attention original ICLR paper\n",
    "        self.W_s1 = nn.Linear(2*hidden_size, 350)\n",
    "        self.W_s2 = nn.Linear(350, 30)\n",
    "        self.fc_layer = nn.Linear(30*2*hidden_size, 2000)\n",
    "        self.label = nn.Linear(2000, output_size)\n",
    "\n",
    "    def attention_net(self, lstm_output):\n",
    "\n",
    "        \"\"\"\n",
    "        Nous allons maintenant utiliser le mécanisme d'auto-attention pour produire une matrice d'embeddings de la phrase d'entrée dans laquelle chaque \n",
    "        ligne représente un encodage de la phrase d'entrée mais en accordant une attention à une partie spécifique de la phrase. \n",
    "        Nous utiliserons 30 de ces embeddings de la phrase d'entrée et enfin nous concaténerons tous les 30 vecteurs d'embedding \n",
    "        de phrases et les connecterons à une couche connectée de taille 2000 qui sera connectée à la couche de sortie de taille 2 renvoyant \n",
    "        des logits pour nos deux classes.\n",
    "        Arguments\n",
    "        ---------\n",
    "        lstm_output = Un tenseur contenant des états cachés correspondant à chaque pas de temps du réseau LSTM.\n",
    "        ---------\n",
    "        Returns : Matrice de pondération de l'attention finale pour les 30 enchaînements de phrases différents dans lesquels chacun des 30 embeddings donne\n",
    "                  une attention aux différentes parties de la phrase d'entrée..\n",
    "        Tensor size : lstm_output.size() = (batch_size, num_seq, 2*hidden_size)\n",
    "                  attn_weight_matrix.size() = (batch_size, 30, num_seq)\n",
    "        \"\"\"\n",
    "        attn_weight_matrix = self.W_s2(torch.tanh(self.W_s1(lstm_output)))\n",
    "        attn_weight_matrix = attn_weight_matrix.permute(0, 2, 1)\n",
    "        attn_weight_matrix = F.softmax(attn_weight_matrix, dim=2)\n",
    "\n",
    "        return attn_weight_matrix\n",
    "\n",
    "    def forward(self, input_sentences, batch_size=None):\n",
    "\n",
    "        \"\"\" \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_sentence: input_sentence taille = (batch_size, num_sequences)\n",
    "        batch_size : default = None. Utilisé uniquement pour la prédiction sur une seule phrase après l'entraînement (batch_size = 1)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Sortie de la couche linéaire contenant les logits pour la classe positive & négative.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        input_ = self.word_embeddings(input_sentences)\n",
    "        input_ = input_.permute(1, 0, 2)\n",
    "        if batch_size is None:\n",
    "            h_0 = Variable(torch.zeros(2, self.batch_size, self.hidden_size).to(device))\n",
    "            c_0 = Variable(torch.zeros(2, self.batch_size, self.hidden_size).to(device))\n",
    "        else:\n",
    "            h_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).cuda())\n",
    "            c_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).cuda())\n",
    "\n",
    "        output, (h_n, c_n) = self.bilstm(input_, (h_0, c_0))\n",
    "        output = output.permute(1, 0, 2)\n",
    "        # output.size() = (batch_size, num_seq, 2*hidden_size)\n",
    "        # h_n.size() = (1, batch_size, hidden_size)\n",
    "        # c_n.size() = (1, batch_size, hidden_size)\n",
    "        attn_weight_matrix = self.attention_net(output)\n",
    "        # attn_weight_matrix.size() = (batch_size, r, num_seq)\n",
    "        # output.size() = (batch_size, num_seq, 2*hidden_size)\n",
    "        hidden_matrix = torch.bmm(attn_weight_matrix, output)\n",
    "        # hidden_matrix.size() = (batch_size, r, 2*hidden_size)\n",
    "        # Let's now concatenate the hidden_matrix and connect it to the fully connected layer.\n",
    "        fc_out = self.fc_layer(hidden_matrix.view(-1, hidden_matrix.size()[1]*hidden_matrix.size()[2]))\n",
    "        logits = self.label(fc_out)\n",
    "        # logits.size() = (batch_size, output_size)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KKT9cOVngV63"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "batch_size = 32\n",
    "output_size = 2 # 2 labels \n",
    "hidden_size = 256 \n",
    "embedding_length = 300 # dimension de GloVe word embeddings\n",
    "word_embeddings = TEXT.vocab.vectors\n",
    "vocab_size = len(TEXT.vocab)\n",
    "dropout = 0.8\n",
    "\n",
    "model = AttentionModel(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings,dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ikFmSEAygryx",
    "outputId": "af9ca58b-4bfa-4352-d057-884ef2a412d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le modèle a 39,559,466 paramètres à entraîner\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'Le modèle a {count_parameters(model):,} paramètres à entraîner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q-TdlrungvVo"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion =  F.cross_entropy\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Retourne l'accuracy par batch\n",
    "    \"\"\"\n",
    "    #arrondi la prédiction à l'entier le plus proche\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zpow3OfQhDO7"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        text = batch.text[0]\n",
    "        target = batch.label \n",
    "\n",
    "        if (text.size()[0] is not batch_size):\n",
    "            continue\n",
    "        predictions = model(text)\n",
    "        target = torch.autograd.Variable(target).long()\n",
    "        loss = criterion(predictions, target)\n",
    "        pred = torch.max(predictions, 1)[1].view(target.size()).data\n",
    "        acc = binary_accuracy(pred.float(), target)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()       \n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "      \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "      \n",
    "    model.eval()\n",
    "      \n",
    "    with torch.no_grad():\n",
    "      \n",
    "        for batch in iterator:\n",
    "\n",
    "            text = batch.text[0]\n",
    "            \n",
    "            if (text.size()[0] is not batch_size):\n",
    "                continue\n",
    "            target = batch.label \n",
    "            predictions = model(text)\n",
    "            target = torch.autograd.Variable(target).long()\n",
    "            loss = criterion(predictions, target )\n",
    "            pred = torch.max(predictions, 1)[1].view(target.size()).data\n",
    "            acc = binary_accuracy(pred.float(), target)\n",
    "              \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "tloss = []\n",
    "tacc = []\n",
    "vloss = []\n",
    "vacc = []\n",
    "\n",
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    tloss.append(train_loss)\n",
    "    tacc.append(train_acc) \n",
    "    \n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    vloss.append(valid_loss)\n",
    "    vacc.append(valid_acc)   \n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut5-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On trouve une valeur d'accuracy d'environ 80% pour les données de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "sns.set()\n",
    "\n",
    "x = np.linspace(0, N_EPOCHS,N_EPOCHS)\n",
    "\n",
    "plt.plot(x,tloss)\n",
    "plt.plot(x,vloss)\n",
    "plt.title(\"Loss\")\n",
    "plt.legend([\"Train loss\", \"Valid loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, N_EPOCHS,N_EPOCHS)\n",
    "\n",
    "plt.plot(x,tacc)\n",
    "plt.plot(x,vacc)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend([\"Train acc\", \"Valid acc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "On teste le modèle sur notre jeu de données test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3-H80W8bhQ2O",
    "outputId": "83726ffe-8c3b-4389-dd8e-303b3a734246"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('tut5-model.pt'))\n",
    "\n",
    "test_loss, test_acc, test_rec, test_f1 = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%| Test Recall: {test_rec*100:.2f}%  | Test F1: {test_f1*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "lAjRJYz4hWgR",
    "outputId": "8d0e2b32-d0a4-4519-97b9-0258b319b74e"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = model(tensor, 1)\n",
    "    out = F.softmax(prediction, 1)\n",
    "    if (torch.argmax(out[0]) == 0):\n",
    "        print (\"Sentiment: Positive\")\n",
    "    else:\n",
    "        print (\"Sentiment: Negative\")\n",
    "\n",
    "predict_sentiment(model, \"this film is not good \")\n",
    "\n",
    "predict_sentiment(model, \"This film is great amazing good \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que contrairement aux modèles précédents ce modèle prend en compte la négation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Références"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM_attention.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
