{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transorfmers_sent_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gutWfaRiaAD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "c56c9123-f01f-4da1-ef5d-46af189dab67"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QWfHCeEi4Fj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_path = \"drive/My Drive/data/\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "patJ7p9J31WT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if False:\n",
        "  data = {}\n",
        "  with open('sentiment-analysis/data/train.csv', newline='') as csvfile:\n",
        "    for i in csv.reader(csvfile):\n",
        "      data.setdefault(i[1], list()).append(i[0])\n",
        "  data.pop('label')\n",
        "  processed = dict((i, list(map(f, j))) for i, j in data.items())\n",
        "  f = lambda x: ' '.join((i[1:-1] if i[0] == i[-1] == \"'\" else i) for i in ''.join((i if (i == \"'\" or i not in string.punctuation) else ' ') for i in x).split())\n",
        "  open('amina.train', 'w').write('\\n'.join('\\n'.join('__label__%d , %s' % (random.choice((1, 3)) if int(label) else random.choice((2, 4)), i) for i in subdata) for label, subdata in processed.items()))\n",
        "\n",
        "if False:\n",
        "  data = {}\n",
        "  with open('sentiment-analysis/data/test.csv', newline='') as csvfile:\n",
        "    for i in csv.reader(csvfile):\n",
        "      data.setdefault(i[1], list()).append(i[0])\n",
        "  data.pop('label')\n",
        "  processed = dict((i, list(map(f, j))) for i, j in data.items())\n",
        "  f = lambda x: ' '.join((i[1:-1] if i[0] == i[-1] == \"'\" else i) for i in ''.join((i if (i == \"'\" or i not in string.punctuation) else ' ') for i in x).split())\n",
        "  open('amina.test', 'w').write('\\n'.join('\\n'.join('__label__%d , %s' % (random.choice((1, 3)) if int(label) else random.choice((2, 4)), i) for i in subdata) for label, subdata in processed.items()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn5bTGVJ0Ltp",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " - Only encoder part of Transformer model is used for classification1\n",
        " - No residual connection, no layer normalization\n",
        " - No need for masking\n",
        " - Multihead attention and positionwise feedforward network to extract features\n",
        " - Then, linear layer to get logits\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnaCA3lY_Erw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "from torchtext.vocab import Vectors, GloVe\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchtext import data\n",
        "\n",
        "# pour la reproductibilité\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "MAX_SEN_LEN = 200\n",
        "TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True, fix_length= MAX_SEN_LEN)\n",
        "LABEL = data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
        "        path=my_path, train='train.csv',\n",
        "        validation='valid.csv', test='test.csv', format='csv', skip_header=True,\n",
        "        fields=[('text', TEXT), ('label', LABEL)])\n",
        "\n",
        "print(f'Taille des données train: {len(train_data)}')\n",
        "print(f'Taille des données de validation: {len(valid_data)}')\n",
        "print(f'Taille des données test: {len(test_data)}')\n",
        "\n",
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "print(f\"Nombre de tokens unique dans le TEXT: {len(TEXT.vocab)}\") \n",
        "print(f\"Nombre unique de LABEL: {len(LABEL.vocab)}\")\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE, sort_key=lambda x: len(x.text), repeat=False, shuffle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa4cvQQDzRzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_utils.py\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "import copy\n",
        "import math\n",
        "\n",
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    '''\n",
        "    Usual Embedding layer with weights multiplied by sqrt(d_model)\n",
        "    '''\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lut(x) * math.sqrt(self.d_model)\n",
        "    \n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
        "                             -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(torch.as_tensor(position.numpy() * div_term.unsqueeze(0).numpy()))\n",
        "        pe[:, 1::2] = torch.cos(torch.as_tensor(position.numpy() * div_term.unsqueeze(0).numpy()))#torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)], \n",
        "                         requires_grad=False)\n",
        "        return self.dropout(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0X0MR4KOzQTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layer normalization module.\"\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
        "    \n",
        "class SublayerOutput(nn.Module):\n",
        "    '''\n",
        "    A residual connection followed by a layer norm.\n",
        "    '''\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerOutput, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SNmjVoVjojP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Positionwise feed-forward network.\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"Implements FFN equation.\"\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD4leCKKsfn5",
        "colab_type": "text"
      },
      "source": [
        "## Encoder\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J68x6UM0jwiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    '''\n",
        "    Transformer Encoder\n",
        "    \n",
        "    It is a stack of N layers.\n",
        "    '''\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "    \n",
        "class EncoderLayer(nn.Module):\n",
        "    '''\n",
        "    An encoder layer\n",
        "    \n",
        "    Made up of self-attention and a feed forward layer.\n",
        "    Each of these sublayers have residual and layer norm, implemented by SublayerOutput.\n",
        "    '''\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer_output = clones(SublayerOutput(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"Transformer Encoder\"\n",
        "        x = self.sublayer_output[0](x, lambda x: self.self_attn(x, x, x, mask)) # Encoder self-attention\n",
        "        return self.sublayer_output[1](x, self.feed_forward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHjBny81sjnA",
        "colab_type": "text"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeEc3epxj2_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    \"Implementation of Scaled dot product attention\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    p_attn = F.softmax(scores, dim = -1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    return torch.matmul(p_attn, value), p_attn\n",
        "\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        \"Take in model size and number of heads.\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"Implements Multi-head attention\"\n",
        "        if mask is not None:\n",
        "            # Same mask applied to all h heads.\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "        \n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
        "        query, key, value = \\\n",
        "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "             for l, x in zip(self.linears, (query, key, value))]\n",
        "        \n",
        "        # 2) Apply attention on all the projected vectors in batch. \n",
        "        x, self.attn = attention(query, key, value, mask=mask, \n",
        "                                 dropout=self.dropout)\n",
        "        \n",
        "        # 3) \"Concat\" using a view and apply a final linear. \n",
        "        x = x.transpose(1, 2).contiguous() \\\n",
        "             .view(nbatches, -1, self.h * self.d_k)\n",
        "        return self.linears[-1](x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuNx9hFsj-Vc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from utils import *\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, N, d_model, d_ff, H, dropout,output_size,batch_size, vocab):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.N = N\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_ff =  d_ff\n",
        "        self.h = h\n",
        "        self.dropout = dropout\n",
        "        self.output_size = output_size\n",
        "        self.batch_size = batch_size  \n",
        "        self.vocab = vocab\n",
        "\n",
        "        h, N, dropout = self.h, self.N, self.dropout\n",
        "        d_model, d_ff = self.d_model, self.d_ff\n",
        "        \n",
        "        attn = MultiHeadedAttention(h, d_model)\n",
        "        ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        position = PositionalEncoding(d_model, dropout)\n",
        "        \n",
        "        self.encoder = Encoder(EncoderLayer(d_model, deepcopy(attn), deepcopy(ff), dropout), N)\n",
        "        self.src_embed = nn.Sequential(Embeddings(d_model, self.vocab), deepcopy(position)) #Embeddings followed by PE\n",
        "\n",
        "        # Fully-Connected Layer\n",
        "        self.fc = nn.Linear(\n",
        "            self.config.d_model,\n",
        "            self.config.output_size\n",
        "        )\n",
        "        \n",
        "        # Softmax non-linearity\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded_sents = self.src_embed(x.permute(1,0)) # shape = (batch_size, sen_len, d_model)\n",
        "        encoded_sents = self.encoder(embedded_sents)\n",
        "        \n",
        "        # Convert input to (batch_size, d_model) for linear layer\n",
        "        final_feature_map = encoded_sents[:,-1,:]\n",
        "        final_out = self.fc(final_feature_map)\n",
        "        return self.softmax(final_out)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SErKLBSpkf7e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "N = 1 #6 in Transformer Paper\n",
        "D_MODEL = 256 #512 in Transformer Paper\n",
        "D_FF = 512 #2048 in Transformer Paper\n",
        "H = 8\n",
        "DROPOUT = 0.1\n",
        "OUTPUT_SIZE = 2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = Transformer(N, D_MODEL,D_FF, H, DROPOUT,OUTPUT_SIZE,BATCH_SIZE, len(TEXT.vocab))\n",
        "  if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "\n",
        "optimizer = optim.Adam(self.parameters())\n",
        "criterion = F.cross_entropy "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RHgAnpNklo7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binary_accuracy(self, preds, y):\n",
        "  \"\"\"\n",
        "  Retourne l'accuracy par batch\n",
        "  \"\"\"\n",
        "  #arrondi la prédiction à l'entier le plus proche\n",
        "  rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "  correct = (rounded_preds == y).float() \n",
        "  acc = correct.sum() / len(correct)\n",
        "  return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLWEXjtDlkPf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, iterator):\n",
        "        \n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "\n",
        "\n",
        "        self.train()\n",
        "    \n",
        "        for batch in iterator:\n",
        "        \n",
        "            optimizer.zero_grad()\n",
        "            text = batch.text[0]\n",
        "            \n",
        "            target = batch.label \n",
        "\n",
        "            if (text.size()[1] is not self.config.batch_size):\n",
        "                continue\n",
        "            predictions = model(text)\n",
        "     \n",
        "            target = torch.autograd.Variable(target).long()\n",
        "            loss = criterion(predictions, target)\n",
        "            #target = [batch_size]\n",
        "            #predictions = [batch_size, output_size]\n",
        "            \n",
        "            pred = torch.max(predictions, 1)[1].view(target.size()).data\n",
        "            #pred = [batch_size]\n",
        "            acc = binary_accuracy(pred.float(), target)\n",
        "\n",
        "            loss.backward()\n",
        "        \n",
        "            optimizer.step()\n",
        "        \n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        " \n",
        "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator):\n",
        "      \n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "        model.eval()\n",
        "      \n",
        "        with torch.no_grad():\n",
        "      \n",
        "            for batch in iterator:\n",
        "\n",
        "                text = batch.text[0]\n",
        "                if (text.size()[1] is not batch_size):# One of the batch returned by BucketIterator has length different than 32.\n",
        "                    continue\n",
        "                target = batch.label \n",
        "                predictions = model(text)\n",
        "     \n",
        "                target = torch.autograd.Variable(target).long()\n",
        "                loss = criterion(predictions, target )\n",
        "                pred = torch.max(predictions, 1)[1].view(target.size()).data\n",
        "            \n",
        "                acc = binary_accuracy(pred.float(), target)\n",
        "        \n",
        "                epoch_loss += loss.item()\n",
        "                epoch_acc += acc.item()\n",
        "\n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hyHUTCKqkCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "\n",
        "N_EPOCHS = 10\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjIYxXa9rO2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss, test_acc = evaluate(model, test_iterator)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%| Test Recall: {test_rec*100:.2f}%  | Test F1: {test_f1*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YIp-J6crPs9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def predict_sentiment(model, sentence):\n",
        "    model.eval()\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    length = [len(indexed)]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(0)\n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    prediction = model(tensor)\n",
        "    out = F.softmax(prediction, 1)\n",
        "    if (torch.argmax(out[0]) == 0):\n",
        "      print (\"Sentiment: Positive\")\n",
        "    else:\n",
        "      print (\"Sentiment: Negative\")\n",
        "\n",
        "predict_sentiment(model, \"This film is terrible\")\n",
        "\n",
        "\n",
        "predict_sentiment(model, \"This film is great amazing good \")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}