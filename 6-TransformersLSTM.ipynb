{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer LSTM\n",
    "\n",
    "En tant que tel, nous allons utiliser la library transformers pour obtenir des transformers pré-entraînés et les utiliser comme couches d'embeddings. Nous allons geler le transformer et entraîner uniquement le reste du modèle qui apprend des représentations produites par le transformer. Dans ce cas, nous utiliserons un LSTM bidirectionnel multicouche."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparer les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le transformer a déjà été formé avec un vocabulaire spécifique, ce qui signifie que nous devons nous entraîner avec exactement le même vocabulaire et également symboliser nos données de la même manière que le transformer l'a fait lors de son apprentissage initial.\n",
    "\n",
    "Le transformer a également été formé avec des tokens spéciaux pour marquer le début et la fin de la phrase. Ainsi qu'avec les tokens : pad et unk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [SEP] [PAD] [UNK]\n"
     ]
    }
   ],
   "source": [
    "len(tokenizer.vocab)\n",
    "\n",
    "init_token = tokenizer.cls_token\n",
    "eos_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "print(init_token, eos_token, pad_token, unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle a été formé sur des séquences d'une longueur maximale définie qui vaut 512. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "101 102 0 100\n"
     ]
    }
   ],
   "source": [
    "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
    "\n",
    "print(max_input_length)\n",
    "def tokenize_and_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    tokens = tokens[:max_input_length-2] # car les tokens de début et de fin deront ajoutés\n",
    "    return tokens\n",
    "\n",
    "init_token_idx = tokenizer.cls_token_id\n",
    "eos_token_idx = tokenizer.sep_token_id\n",
    "pad_token_idx = tokenizer.pad_token_id\n",
    "unk_token_idx = tokenizer.unk_token_id\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)\n",
    "\n",
    "from torchtext import data\n",
    "\n",
    "TEXT = data.Field(batch_first = True, # le transformer s'attend à ce que la dimension du lot soit la première,\n",
    "                  use_vocab = False, # nous avons déjà le vocabulaire de notre texte, fourni par le transformer\n",
    "                  tokenize = tokenize_and_cut,\n",
    "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                  init_token = init_token_idx,\n",
    "                  eos_token = eos_token_idx,\n",
    "                  pad_token = pad_token_idx,\n",
    "                  unk_token = unk_token_idx)\n",
    "\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille des données train: 18163\n",
      "Taille des données de validation: 2270\n",
      "Taille des données test: 2271\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
    "        path='./data/', train='train.csv',\n",
    "        validation='valid.csv', test='test.csv', format='csv', skip_header=True,\n",
    "        fields=[('text', TEXT), ('label', LABEL)])\n",
    "\n",
    "print(f'Taille des données train: {len(train_data)}')\n",
    "print(f'Taille des données de validation: {len(valid_data)}')\n",
    "print(f'Taille des données test: {len(test_data)}')\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device, sort = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construire le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au lieu d'utiliser une couche embedding pour obtenir des embedding pour notre texte, nous utiliserons le modèle de transformer pré-entraîné. Ces embeddings seront ensuite introduites dans un LSTM pour produire une prédiction du sentiment de la phrase d'entrée. Nous obtenons la taille de la dimension d'embedding (appelée hidden_size) du transformer via son attribut de configuration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BERTGRUSentiment(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 n_layers,\n",
    "                 bidirectional,\n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()       \n",
    "        self.bert = bert  \n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']     \n",
    "        self.rnn = nn.LSTM(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          bidirectional = bidirectional,\n",
    "                          batch_first = True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout)\n",
    "        \n",
    "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)   \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [batch size, sent len]\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            embedded = self.bert(text)[0]\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        _, hidden = self.rnn(embedded)\n",
    "        \n",
    "       \n",
    "        hidden = hidden[0]\n",
    "        #hidden = [n layers * n directions, batch size, emb dim]\n",
    "  \n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "                \n",
    "        #hidden = [batch size, hid dim]\n",
    "        \n",
    "        output = self.out(hidden)\n",
    "        \n",
    "        #output = [batch size, out dim]\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.25\n",
    "\n",
    "model = BERTGRUSentiment(bert,\n",
    "                         HIDDEN_DIM,\n",
    "                         OUTPUT_DIM,\n",
    "                         N_LAYERS,\n",
    "                         BIDIRECTIONAL,\n",
    "                         DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le modèle a 113,160,961 paramètres à entraîner\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'Le modèle a {count_parameters(model):,} paramètres à entraîner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de figer les paramètres (et non de les entraîner), nous devons définir leur attribut requires_grad sur False. Pour ce faire, nous parcourons simplement tous les named_parameters de notre modèle et s'ils font partie du modèle de transformer de bert, nous définissons require_grad = False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 3,678,721 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():                \n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = False\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient ainsi un nombre de paramètres semblables à nos autres modèles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def recall(preds, y):\n",
    "    '''\n",
    "    Retourne le recall\n",
    "    '''\n",
    "    y_pred = torch.round(torch.sigmoid(preds))\n",
    "    y_true = (y_pred == y).float()       \n",
    "    \n",
    "    tp = (y_true * y_pred).sum().float()\n",
    "    tn = ((1 - y_true) * (1 - y_pred)).sum().float()\n",
    "    fp = ((1 - y_true) * y_pred).sum().float()\n",
    "    fn = (y_true * (1 - y_pred)).sum().float()\n",
    "    \n",
    "    if (tp + fn) == 0:\n",
    "        recall = torch.zeros(1)\n",
    "        \n",
    "    recall = tp / (tp + fn)\n",
    "    return recall\n",
    "\n",
    "def f1_loss(preds, y):\n",
    "    '''\n",
    "    Retourne le score F1\n",
    "    '''  \n",
    "    y_pred = torch.round(torch.sigmoid(preds))\n",
    "    y_true = (y_pred == y).float() \n",
    "            \n",
    "    tp = (y_true * y_pred).sum().float()\n",
    "    tn = ((1 - y_true) * (1 - y_pred)).sum().float()\n",
    "    fp = ((1 - y_true) * y_pred).sum().float()\n",
    "    fn = (y_true * (1 - y_pred)).sum().float()\n",
    "    \n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    \n",
    "    if (tp + fn) == 0 or (tp + fp) == 0 or (recall + precision == 0):\n",
    "        f1 = torch.zeros(1)\n",
    "    else:\n",
    "        f1 = 2* (precision*recall) / (precision + recall)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_rec = 0\n",
    "    epoch_f1 = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        rec = recall(predictions, batch.label)\n",
    "        f1 = f1_loss(predictions, batch.label)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_rec += rec.item()\n",
    "        epoch_f1 += f1.item() \n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_rec / len(iterator), epoch_f1 / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_rec = 0\n",
    "    epoch_f1 = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            rec = recall(predictions, batch.label)\n",
    "            f1 = f1_loss(predictions, batch.label)\n",
    "              \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_rec += rec.item()\n",
    "            epoch_f1 += f1.item() \n",
    "            \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_rec / len(iterator), epoch_f1 / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 6m 32s\n",
      "\tTrain Loss: 0.521 | Train Acc: 74.04% | Train Recall: 40.50% | Train F1: 49.88%\n",
      "\t Val. Loss: 0.472 |  Val. Acc: 76.39% | Val. Recall: 32.73%  | Val. F1: 47.64%\n",
      "Epoch: 02 | Epoch Time: 8m 57s\n",
      "\tTrain Loss: 0.439 | Train Acc: 79.18% | Train Recall: 45.16% | Train F1: 55.15%\n",
      "\t Val. Loss: 0.298 |  Val. Acc: 88.23% | Val. Recall: 44.40%  | Val. F1: 58.77%\n",
      "Epoch: 03 | Epoch Time: 6m 47s\n",
      "\tTrain Loss: 0.290 | Train Acc: 88.00% | Train Recall: 44.84% | Train F1: 58.90%\n",
      "\t Val. Loss: 0.241 |  Val. Acc: 90.36% | Val. Recall: 45.24%  | Val. F1: 59.79%\n",
      "Epoch: 04 | Epoch Time: 6m 37s\n",
      "\tTrain Loss: 0.252 | Train Acc: 90.10% | Train Recall: 44.76% | Train F1: 59.43%\n",
      "\t Val. Loss: 0.244 |  Val. Acc: 90.79% | Val. Recall: 45.05%  | Val. F1: 59.79%\n",
      "Epoch: 05 | Epoch Time: 6m 38s\n",
      "\tTrain Loss: 0.235 | Train Acc: 90.52% | Train Recall: 44.76% | Train F1: 59.55%\n",
      "\t Val. Loss: 0.229 |  Val. Acc: 91.23% | Val. Recall: 44.81%  | Val. F1: 59.78%\n",
      "Epoch: 06 | Epoch Time: 6m 38s\n",
      "\tTrain Loss: 0.216 | Train Acc: 91.42% | Train Recall: 44.82% | Train F1: 59.86%\n",
      "\t Val. Loss: 0.233 |  Val. Acc: 90.83% | Val. Recall: 45.29%  | Val. F1: 59.91%\n",
      "Epoch: 07 | Epoch Time: 6m 34s\n",
      "\tTrain Loss: 0.205 | Train Acc: 91.76% | Train Recall: 44.76% | Train F1: 59.93%\n",
      "\t Val. Loss: 0.229 |  Val. Acc: 91.14% | Val. Recall: 43.65%  | Val. F1: 59.11%\n",
      "Epoch: 08 | Epoch Time: 6m 36s\n",
      "\tTrain Loss: 0.184 | Train Acc: 92.77% | Train Recall: 44.77% | Train F1: 60.20%\n",
      "\t Val. Loss: 0.233 |  Val. Acc: 91.23% | Val. Recall: 43.49%  | Val. F1: 59.06%\n",
      "Epoch: 09 | Epoch Time: 6m 33s\n",
      "\tTrain Loss: 0.167 | Train Acc: 93.60% | Train Recall: 44.90% | Train F1: 60.49%\n",
      "\t Val. Loss: 0.240 |  Val. Acc: 91.64% | Val. Recall: 46.19%  | Val. F1: 60.60%\n",
      "Epoch: 10 | Epoch Time: 6m 34s\n",
      "\tTrain Loss: 0.151 | Train Acc: 94.09% | Train Recall: 45.00% | Train F1: 60.67%\n",
      "\t Val. Loss: 0.239 |  Val. Acc: 91.13% | Val. Recall: 44.32%  | Val. F1: 59.49%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc, train_rec, train_f1 = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc, valid_rec, valid_f1 = evaluate(model, valid_iterator, criterion)\n",
    "        \n",
    "    end_time = time.time()\n",
    "        \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut6-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Train Recall: {train_rec*100:.2f}% | Train F1: {train_f1*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% | Val. Recall: {valid_rec*100:.2f}%  | Val. F1: {valid_f1*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.256 | Test Acc: 90.31%| Test Recall: 47.00%  | Test F1: 61.73%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut6-model.pt'))\n",
    "test_loss, test_acc, test_rec, test_f1 = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%| Test Recall: {test_rec*100:.2f}%  | Test F1: {test_f1*100:.2f}%')\n",
    "\n",
    "def predict_sentiment(model, tokenizer, sentence):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9925274848937988"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, tokenizer, \"This film is terrible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008738343603909016"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, tokenizer, \"This film is great\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Référence : \n",
    "\n",
    " - https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/6%20-%20Transformers%20for%20Sentiment%20Analysis.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
