{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtoDrTNFfyx1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "b58108f0-766d-4782-ad22-aa4e5ceb2825"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifdYYtURf2pB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_path = \"drive/My Drive/data/\""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7eAcLKmQWxa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "e548134d-26bf-40cd-9382-aad14446256c"
      },
      "source": [
        "\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "from torchtext.vocab import Vectors, GloVe\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchtext import data\n",
        "\n",
        "# pour la reproductibilité\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "TEXT = data.Field(sequential=True,lower=True, tokenize = 'spacy', include_lengths=True, batch_first=True, fix_length=200)\n",
        "LABEL = data.LabelField(dtype = torch.float)\n",
        "\n",
        "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
        "        path=my_path, train='train.csv',\n",
        "        validation='valid.csv', test='test.csv', format='csv', skip_header=True,\n",
        "        fields=[('text', TEXT), ('label', LABEL)])\n",
        "\n",
        "print(f'Taille des données train: {len(train_data)}')\n",
        "print(f'Taille des données de validation: {len(valid_data)}')\n",
        "print(f'Taille des données test: {len(test_data)}')\n",
        "\n",
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE, vectors=GloVe(name='6B', dim=300))\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "print(f\"Nombre de tokens unique dans le TEXT: {len(TEXT.vocab)}\") \n",
        "print(f\"Nombre unique de LABEL: {len(LABEL.vocab)}\")\n",
        "\n",
        "\n",
        "# utilisation du GPU si possible \n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "device = device, sort_key=lambda x: len(x.text), repeat=False, shuffle=True)\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Taille des données train: 18163\n",
            "Taille des données de validation: 2270\n",
            "Taille des données test: 2271\n",
            "Nombre de tokens unique dans le TEXT: 25002\n",
            "Nombre unique de LABEL: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AE1F3rGQX2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttentionModel(torch.nn.Module):\n",
        "\tdef __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
        "\t\tsuper(AttentionModel, self).__init__()\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\t\tArguments\t\n",
        "\t\t---------\n",
        "\t\tbatch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
        "\t\toutput_size : 2 = (pos, neg)\n",
        "\t\thidden_sie : Size of the hidden_state of the LSTM\n",
        "\t\tvocab_size : Size of the vocabulary containing unique words\n",
        "\t\tembedding_length : Embeddding dimension of GloVe word embeddings\n",
        "\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
        "\t\t\n",
        "\t\t--------\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\t\t\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.output_size = output_size\n",
        "\t\tself.hidden_size = hidden_size\n",
        "\t\tself.vocab_size = vocab_size\n",
        "\t\tself.embedding_length = embedding_length\n",
        "\t\t\n",
        "\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
        "\t\tself.word_embeddings.weights = nn.Parameter(weights, requires_grad=False)\n",
        "\t\tself.lstm = nn.LSTM(embedding_length, hidden_size)\n",
        "\t\tself.label = nn.Linear(hidden_size, output_size)\n",
        "\t\t\n",
        "\tdef attention_net(self, lstm_output, final_state):\n",
        "\n",
        "\t\t\"\"\" \n",
        "\t\tNow we will incorporate Attention mechanism in our LSTM model. In this new model, we will use attention \n",
        "    to compute soft alignment score corresponding\n",
        "\t\tbetween each of the hidden_state and the last hidden_state of the LSTM. We will be using torch.bmm \n",
        "    for the batch matrix multiplication.\n",
        "\t\t\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\t\n",
        "\t\tlstm_output : Final output of the LSTM which contains hidden layer outputs for each sequence.\n",
        "\t\tfinal_state : Final time-step hidden state (h_n) of the LSTM\n",
        "\t\t\n",
        "\t\t---------\n",
        "\t\t\n",
        "\t\tReturns : It performs attention mechanism by first computing weights for each of \n",
        "    the sequence present in lstm_output and and then finally computing the\n",
        "\t\t\t\t  new hidden state.\n",
        "\t\t\t\t  \n",
        "\t\tTensor Size :\n",
        "\t\t\t\t\thidden.size() = (batch_size, hidden_size)\n",
        "\t\t\t\t\tattn_weights.size() = (batch_size, num_seq)\n",
        "\t\t\t\t\tsoft_attn_weights.size() = (batch_size, num_seq)\n",
        "\t\t\t\t\tnew_hidden_state.size() = (batch_size, hidden_size)\n",
        "\t\t\t\t\t  \n",
        "\t\t\"\"\"\n",
        "\t\t\n",
        "\t\thidden = final_state.squeeze(0)\n",
        "\t\tattn_weights = torch.bmm(lstm_output, hidden.unsqueeze(2)).squeeze(2)\n",
        "\t\tsoft_attn_weights = F.softmax(attn_weights, 1)\n",
        "\t\tnew_hidden_state = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
        "\t\t\n",
        "\t\treturn new_hidden_state\n",
        "\t\n",
        "\tdef forward(self, input_sentences, batch_size=None):\n",
        "\t\n",
        "\t\t\"\"\" \n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tinput_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
        "\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
        "\t\t\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tOutput of the linear layer containing logits for pos & neg class which receives its input as the new_hidden_state which is basically the output of the Attention network.\n",
        "\t\tfinal_output.shape = (batch_size, output_size)\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\t\t\n",
        "\t\tinput = self.word_embeddings(input_sentences)\n",
        "\t\tinput = input.permute(1, 0, 2)\n",
        "\t\tif batch_size is None:\n",
        "\t\t\th_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda())\n",
        "\t\t\tc_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda())\n",
        "\t\telse:\n",
        "\t\t\th_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
        "\t\t\tc_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
        "\t\t\t\n",
        "\t\toutput, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0)) # final_hidden_state.size() = (1, batch_size, hidden_size) \n",
        "\t\toutput = output.permute(1, 0, 2) # output.size() = (batch_size, num_seq, hidden_size)\n",
        "\t\t\n",
        "\t\tattn_output = self.attention_net(output, final_hidden_state)\n",
        "\t\tlogits = self.label(attn_output)\n",
        "\t\t\n",
        "\t\treturn logits"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKT9cOVngV63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "learning_rate = 2e-5\n",
        "batch_size = 32\n",
        "output_size = 2\n",
        "hidden_size = 256\n",
        "embedding_length = 300\n",
        "word_embeddings = TEXT.vocab.vectors\n",
        "vocab_size = len(TEXT.vocab)\n",
        "\n",
        "model = AttentionModel(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikFmSEAygryx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "af9ca58b-4bfa-4352-d057-884ef2a412d4"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'Le modèle a {count_parameters(model):,} paramètres à entraîner')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Le modèle a 8,072,506 paramètres à entraîner\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-TdlrungvVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion =  F.cross_entropy\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Retourne l'accuracy par batch\n",
        "    \"\"\"\n",
        "    #arrondi la prédiction à l'entier le plus proche\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "def recall(preds, y):\n",
        "    '''\n",
        "    Retourne le recall\n",
        "    '''\n",
        "    y_pred = torch.round(torch.sigmoid(preds))\n",
        "    y_true = (y_pred == y).float()       \n",
        "    \n",
        "    tp = (y_true * y_pred).sum().float()\n",
        "    tn = ((1 - y_true) * (1 - y_pred)).sum().float()\n",
        "    fp = ((1 - y_true) * y_pred).sum().float()\n",
        "    fn = (y_true * (1 - y_pred)).sum().float()\n",
        "    \n",
        "    if (tp + fn) == 0:\n",
        "        recall = torch.zeros(1)\n",
        "        \n",
        "    recall = tp / (tp + fn)\n",
        "    return recall\n",
        "\n",
        "\n",
        "\n",
        "def f1_loss(preds, y):\n",
        "    '''\n",
        "    Retourne le score F1\n",
        "    '''  \n",
        "    y_pred = torch.round(torch.sigmoid(preds))\n",
        "    y_true = (y_pred == y).float() \n",
        "            \n",
        "    tp = (y_true * y_pred).sum().float()\n",
        "    tn = ((1 - y_true) * (1 - y_pred)).sum().float()\n",
        "    fp = ((1 - y_true) * y_pred).sum().float()\n",
        "    fn = (y_true * (1 - y_pred)).sum().float()\n",
        "    \n",
        "    recall = tp / (tp + fn)\n",
        "    precision = tp / (tp + fp)\n",
        "    \n",
        "    if (tp + fn) == 0 or (tp + fp) == 0 or (recall + precision == 0):\n",
        "        f1 = torch.zeros(1)\n",
        "    else:\n",
        "        f1 = 2* (precision*recall) / (precision + recall)\n",
        "    \n",
        "    return f1"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zpow3OfQhDO7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "73fa0741-c3ec-42b0-8be6-42326d9e8654"
      },
      "source": [
        "\n",
        "\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    epoch_rec = 0\n",
        "    epoch_f1 = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        text = batch.text[0]\n",
        "        target = batch.label \n",
        "\n",
        "        \n",
        "        if (text.size()[0] is not 32):# One of the batch returned by BucketIterator has length different than 32.\n",
        "            continue\n",
        "        predictions = model(text)\n",
        "        target = torch.autograd.Variable(target).long()\n",
        "        loss = criterion(predictions, target )\n",
        "        #target = [batch_size]\n",
        "        #predictions = [batch_size, output_size]\n",
        "        pred = torch.max(predictions, 1)[1].view(target.size()).data\n",
        "        #pred = [batch_size]\n",
        "        acc = binary_accuracy(pred.float(), target)\n",
        "        rec = recall(pred.float(), target)\n",
        "        f1 = f1_loss(pred.float(), target)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        epoch_rec += rec.item()\n",
        "        epoch_f1 += f1.item()        \n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_rec / len(iterator), epoch_f1 / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    epoch_rec = 0\n",
        "    epoch_f1 = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "          text = batch.text[0]\n",
        "          if (text.size()[0] is not 32):# One of the batch returned by BucketIterator has length different than 32.\n",
        "            continue\n",
        "          target = batch.label \n",
        "          predictions = model(text)\n",
        "          target = torch.autograd.Variable(target).long()\n",
        "          loss = criterion(predictions, target )\n",
        "          pred = torch.max(predictions, 1)[1].view(target.size()).data\n",
        "          #num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).float().sum()\n",
        "          #acc = 100.0 * num_corrects/len(batch)\n",
        "          acc = binary_accuracy(pred.float(), target)\n",
        "          rec = recall(pred.float(), target)\n",
        "          f1 = f1_loss(pred.float(), target)\n",
        "            \n",
        "          epoch_loss += loss.item()\n",
        "          epoch_acc += acc.item()\n",
        "          epoch_rec += rec.item()\n",
        "          epoch_f1 += f1.item() \n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_rec / len(iterator), epoch_f1 / len(iterator)\n",
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "\n",
        "N_EPOCHS = 10\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc, train_rec, train_f1 = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc, valid_rec, valid_f1 = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Train Recall: {train_rec*100:.2f}% | Train F1: {train_f1*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% | Val. Recall: {valid_rec*100:.2f}%  | Val. F1: {valid_f1*100:.2f}%')\n",
        "\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.348 | Train Acc: 85.14% | Train Recall: 43.87% | Train F1: 57.18%\n",
            "\t Val. Loss: 0.391 |  Val. Acc: 81.56% | Val. Recall: 42.31%  | Val. F1: 54.98%\n",
            "Epoch: 02 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.216 | Train Acc: 91.66% | Train Recall: 44.61% | Train F1: 59.37%\n",
            "\t Val. Loss: 0.411 |  Val. Acc: 82.26% | Val. Recall: 43.79%  | Val. F1: 55.99%\n",
            "Epoch: 03 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.119 | Train Acc: 95.65% | Train Recall: 45.06% | Train F1: 60.68%\n",
            "\t Val. Loss: 0.521 |  Val. Acc: 82.09% | Val. Recall: 45.10%  | Val. F1: 56.70%\n",
            "Epoch: 04 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.061 | Train Acc: 97.88% | Train Recall: 45.15% | Train F1: 61.27%\n",
            "\t Val. Loss: 0.590 |  Val. Acc: 81.69% | Val. Recall: 45.40%  | Val. F1: 56.75%\n",
            "Epoch: 05 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.027 | Train Acc: 99.04% | Train Recall: 45.25% | Train F1: 61.57%\n",
            "\t Val. Loss: 0.684 |  Val. Acc: 82.39% | Val. Recall: 43.47%  | Val. F1: 55.93%\n",
            "Epoch: 06 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.023 | Train Acc: 99.16% | Train Recall: 45.22% | Train F1: 61.61%\n",
            "\t Val. Loss: 0.897 |  Val. Acc: 80.50% | Val. Recall: 47.90%  | Val. F1: 57.77%\n",
            "Epoch: 07 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.015 | Train Acc: 99.43% | Train Recall: 45.26% | Train F1: 61.69%\n",
            "\t Val. Loss: 0.861 |  Val. Acc: 82.17% | Val. Recall: 41.89%  | Val. F1: 54.85%\n",
            "Epoch: 08 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.019 | Train Acc: 99.29% | Train Recall: 45.29% | Train F1: 61.71%\n",
            "\t Val. Loss: 0.871 |  Val. Acc: 81.38% | Val. Recall: 43.50%  | Val. F1: 55.56%\n",
            "Epoch: 09 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.012 | Train Acc: 99.49% | Train Recall: 45.28% | Train F1: 61.70%\n",
            "\t Val. Loss: 0.921 |  Val. Acc: 82.04% | Val. Recall: 42.29%  | Val. F1: 55.00%\n",
            "Epoch: 10 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.008 | Train Acc: 99.59% | Train Recall: 45.27% | Train F1: 61.73%\n",
            "\t Val. Loss: 1.055 |  Val. Acc: 81.65% | Val. Recall: 45.22%  | Val. F1: 56.67%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-H80W8bhQ2O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "83726ffe-8c3b-4389-dd8e-303b3a734246"
      },
      "source": [
        "\n",
        "test_loss, test_acc, test_rec, test_f1 = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%| Test Recall: {test_rec*100:.2f}%  | Test F1: {test_f1*100:.2f}%')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.974 | Test Acc: 83.54%| Test Recall: 47.19%  | Test F1: 59.39%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAjRJYz4hWgR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "30572d20-d01a-4942-e8fa-31b8f90a2c05"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def predict_sentiment(model, sentence):\n",
        "    model.eval()\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    length = [len(indexed)]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(0)\n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    prediction = model(tensor, 1)\n",
        "    out = F.softmax(prediction, 1)\n",
        "    if (torch.argmax(out[0]) == 0):\n",
        "      print (\"Sentiment: Positive\")\n",
        "    else:\n",
        "      print (\"Sentiment: Negative\")\n",
        "\n",
        "predict_sentiment(model, \"This film is horrible bad bad\")\n",
        "\n",
        "\n",
        "predict_sentiment(model, \"This film is great amazing good \")\n",
        "\n"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3.4362e-08, 1.0000e+00], device='cuda:0', grad_fn=<SelectBackward>)\n",
            "Sentiment: Negative\n",
            "tensor([1.0000e+00, 4.1757e-07], device='cuda:0', grad_fn=<SelectBackward>)\n",
            "Sentiment: Positive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}