{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "\n",
    "Les couches convolutives utilisent des filtres qui balaient une image et produisent une version traitée de l'image. Cette version traitée de l'image peut être introduite dans une autre couche convolutionnelle ou une couche linéaire. Chaque filtre a une forme, par ex. un filtre 3x3 couvre une zone de 3 pixels de large et 3 pixels de haut de l'image, et chaque élément du filtre a un poids qui lui est associé, le filtre 3x3 aurait 9 poids. Dans le traitement d'image traditionnel, ces poids ont été spécifiés à la main par des ingénieurs, mais le principal avantage des couches convolutives dans les réseaux de neurones est que ces poids sont appris par rétropropagation.\n",
    "\n",
    "L'idée intuitive derrière l'apprentissage des poids est que les couches convolutives agissent comme des extracteurs de features, extrayant les parties de l'image les plus importantes pour l'objectif du CNN. Par exemple, si vous utilisez un CNN pour détecter des visages dans une image, le CNN peut rechercher des caractéristiques telles que l'existence d'un nez, d'une bouche ou d'une paire d'yeux dans l'image.\n",
    "\n",
    "De la même manière qu'un filtre 3x3 peut regarder sur une image, un filtre 1x2 peut regarder sur 2 mots séquentiels dans un morceau de texte, c'est-à-dire un bi-gramme. Dans ce modèle CNN, nous utiliserons plusieurs filtres de tailles différentes qui examineront les bi-grammes (un 1x2 filter), tri-grammes (un filtre 1x3) et / ou n-grammes (un 1x $ n $ filtre) dans le texte.\n",
    "\n",
    "L'intuition ici est que l'apparition de certains bi-grammes, tri-grammes et n-grammes dans la revue sera une bonne indication du sentiment final.\n",
    "\n",
    "## Préparer les données\n",
    "\n",
    "Comme les couches convolutionnelles s'attendent à ce que la dimension du batch soit la première, nous pouvons dire à TorchText de renvoyer les données déjà permutées en utilisant l'argument batch_first = True dans la méthode `Field`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille des données train: 25000\n",
      "Taille des données de validation: 12500\n",
      "Taille des données test: 12500\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(batch_first = True)\n",
    "LABEL = data.LabelField(dtype = torch.float)\n",
    "\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "test_data, valid_data = torch.split(test_data, 0.5, dim=1) \n",
    "\n",
    "\n",
    "print(f'Taille des données train: {len(train_data)}')\n",
    "print(f'Taille des données de validation: {len(valid_data)}')\n",
    "print(f'Taille des données test: {len(test_data)}')\n",
    "\n",
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Itérateurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device, sort = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construire le modèle\n",
    "\n",
    "Le texte est en 1 dimension. Cependant, nous savons que la première étape de presque tous nos notebooks précédents consiste à convertir les mots en embedding. C'est ainsi que nous pouvons visualiser nos mots en 2 dimensions, chaque mot le long d'un axe et les éléments de vecteurs sur l'autre dimension.\n",
    "On peut alors utiliser un filtre qui est **[n x emb_dim]**. Cela couvrira entièrement les mots séquentiels $ n $, car leur largeur sera de dimension emb_dim.\n",
    "\n",
    "La prochaine étape de notre modèle consiste à utiliser le pooling (en particulier le pooling max) sur la sortie des couches convolutives afin de prendre la valeur maximale sur une dimension.\n",
    "\n",
    " - Nous implémentons les couches convolutives avec nn.Conv2d. L'argument in_channels est le nombre de \"channels\" nous n'avons qu'un seul \"channel\", le texte lui-même. Le out_channels est le nombre de filtres et le kernel_size est la taille des filtres. Chacun de nos kernel_sizes va être [n x emb_dim] où $ n $ est la taille des n-grammes.\n",
    " - En PyTorch, les RNN veulent l'entrée avec la dimension batch en second, tandis que les CNN veulent d'abord la dimension batch - nous n'avons pas besoin de permuter les données ici car nous avons déjà défini batch_first = True dans notre champ TEXT.\n",
    " - Nous passons ensuite les tenseurs à travers les couches convolutives et de pooling, en utilisant la fonction d'activation ReLU après les couches convolutives. La taille de la sortie de la couche convolutive dépend de la taille de l'entrée, et différents lots contiennent des phrases de différentes longueurs.\n",
    " - Enfin, nous effectuons des dropouts sur les sorties de filtre concaténées, puis nous les faisons passer à travers une couche linéaire pour faire nos prédictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le modèle a 2,590,801 paramètres à entraîner\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "                \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)  \n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = (fs, embedding_dim)) \n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)  \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):           \n",
    "        \n",
    "        embedded = self.dropout(self.embedding(text)) \n",
    "        embedded = embedded.unsqueeze(1) \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]        \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved] \n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "            \n",
    "        return self.fc(cat)\n",
    "    \n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [2,3,4]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'Le modèle a {count_parameters(model):,} paramètres à entraîner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Retourne l'accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 31m 15s\n",
      "\tTrain Loss: 0.718 | Train Acc: 54.13%\n",
      "\t Val. Loss: 0.655 |  Val. Acc: 63.51%\n",
      "Epoch: 02 | Epoch Time: 37m 12s\n",
      "\tTrain Loss: 0.660 | Train Acc: 60.55%\n",
      "\t Val. Loss: 0.601 |  Val. Acc: 67.81%\n",
      "Epoch: 03 | Epoch Time: 48m 4s\n",
      "\tTrain Loss: 0.576 | Train Acc: 69.68%\n",
      "\t Val. Loss: 0.460 |  Val. Acc: 78.92%\n",
      "Epoch: 04 | Epoch Time: 33m 28s\n",
      "\tTrain Loss: 0.479 | Train Acc: 76.88%\n",
      "\t Val. Loss: 0.392 |  Val. Acc: 82.67%\n",
      "Epoch: 05 | Epoch Time: 32m 32s\n",
      "\tTrain Loss: 0.417 | Train Acc: 80.93%\n",
      "\t Val. Loss: 0.363 |  Val. Acc: 84.19%\n",
      "Epoch: 06 | Epoch Time: 32m 22s\n",
      "\tTrain Loss: 0.377 | Train Acc: 83.16%\n",
      "\t Val. Loss: 0.345 |  Val. Acc: 84.80%\n",
      "Epoch: 07 | Epoch Time: 31m 57s\n",
      "\tTrain Loss: 0.348 | Train Acc: 84.66%\n",
      "\t Val. Loss: 0.333 |  Val. Acc: 85.56%\n",
      "Epoch: 08 | Epoch Time: 49m 27s\n",
      "\tTrain Loss: 0.318 | Train Acc: 86.48%\n",
      "\t Val. Loss: 0.324 |  Val. Acc: 86.20%\n"
     ]
    }
   ],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()  \n",
    "        predictions = model(batch.text).squeeze(1)  \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()    \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "tloss = []\n",
    "tacc = []\n",
    "vloss = []\n",
    "vacc = []\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)   \n",
    "    tloss.append(train_loss)\n",
    "    tacc.append(train_acc)   \n",
    "    \n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    vloss.append(valid_loss)\n",
    "    vacc.append(valid_acc)    \n",
    "\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut4-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "sns.set()\n",
    "\n",
    "x = np.linspace(0, N_EPOCHS,N_EPOCHS)\n",
    "\n",
    "plt.plot(x,tloss)\n",
    "plt.plot(x,vloss)\n",
    "plt.title(\"Loss\")\n",
    "plt.legend([\"Train loss\", \"Valid loss\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, N_EPOCHS,N_EPOCHS)\n",
    "\n",
    "plt.plot(x,tacc)\n",
    "plt.plot(x,vacc)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend([\"Train acc\", \"Valid acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('tut4-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment(model, sentence, min_len = 5):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    if len(tokenized) < min_len:\n",
    "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    \n",
    "    return prediction.item()\n",
    "def predict_sentiment(model, sentence, min_len = 5):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = model(tensor, 1)\n",
    "    out = F.softmax(prediction, 1)\n",
    "    if (torch.argmax(out[0]) == 0):\n",
    "        print (\"Sentiment: Positive\")\n",
    "    else:\n",
    "        print (\"Sentiment: Negative\")\n",
    "\n",
    "predict_sentiment(model, \"this film is not good\")\n",
    "\n",
    "predict_sentiment(model, \"This film is great amazing good \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Références :\n",
    "\n",
    " - https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb\n",
    " - https://arxiv.org/pdf/1408.5882.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
