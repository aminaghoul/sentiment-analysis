{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM \n",
    "\n",
    "## Importer et préparer des données\n",
    "\n",
    "Nous utiliserons des séquences remplies, ce qui fera que notre RNN ne traitera que les éléments non remplis de notre séquence, et pour tout élément rempli, la sortie sera un tenseur nul. Pour utiliser des séquences rembourrées, nous devons indiquer au RNN la longueur des séquences réelles. Nous faisons cela en définissant include_lengths = True pour notre champ TEXT. Cela fera que batch.text sera maintenant un tuple avec le premier élément étant notre phrase (un tenseur numérisé qui a été complété) et le deuxième élément étant les longueurs réelles de nos phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille des données train: 18163\n",
      "Taille des données de validation: 2270\n",
      "Taille des données test: 2271\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def generate_bigrams(x):\n",
    "    n_grams = set(zip(*[x[i:] for i in range(2)]))\n",
    "    for n_gram in n_grams:\n",
    "        x.append(' '.join(n_gram))\n",
    "    return x\n",
    "\n",
    "TEXT = data.Field(sequential=True,lower=True, tokenize = 'spacy', preprocessing = generate_bigrams, \n",
    "                  include_lengths=True)\n",
    "LABEL = data.LabelField(dtype = torch.float)\n",
    "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
    "        path='./data/', train='train.csv',\n",
    "        validation='valid.csv', test='test.csv', format='csv', skip_header=True,\n",
    "        fields=[('text', TEXT), ('label', LABEL)])\n",
    "\n",
    "print(f'Taille des données train: {len(train_data)}')\n",
    "print(f'Taille des données de validation: {len(valid_data)}')\n",
    "print(f'Taille des données test: {len(test_data)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulaire\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vient ensuite l'utilisation de word embeding pré-entraînées. Maintenant, au lieu d'avoir nos embeddings de mots initialisés au hasard, ils sont initialisés avec ces vecteurs pré-entraînés. Nous obtenons ces vecteurs simplement en spécifiant quels vecteurs nous voulons et en les passant comme argument à build_vocab. TorchText gère le téléchargement des vecteurs et les associe aux mots corrects de notre vocabulaire.\n",
    "\n",
    "Ici, nous allons utiliser les \"vecteurs\" glove.6B.100d. Glove est l'algorithme utilisé pour calculer les vecteurs, allez ici pour en savoir plus. 6B indique que ces vecteurs ont été entraînés sur 6 milliards de jetons et 100d indique que ces vecteurs sont 100 -dimensionnelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Itérateur\n",
    "\n",
    "Pour les séquences rembourrées emballées, tous les tenseurs d'un lot doivent être triés par leurs longueurs. Ceci est géré dans l'itérateur en définissant sort_within_batch = True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device,sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle\n",
    "\n",
    "Dans ce modèle, on a :\n",
    "\n",
    " - LSTM au lieu de RNN. En effet, les RNN standard souffrent du problème de disparition du gradient. Les LSTM surmontent ce problème en ayant un état récurrent supplémentaire appelé cellule, $ c $  qui peut être considéré comme la «mémoire» du LSTM et en utilisant plusieurs portes (gates) qui contrôlent le flux d'informations dans et hors de la mémoire. Nous pouvons simplement considérer le LSTM comme une fonction de $ x_t $, $ h_t $ et $ c_t $, au lieu de seulement $ x_t $ et $ h_t $. $$(h_t, c_t) = \\text{LSTM}(x_t, h_t, c_t)$$L'état initial de la cellule, $ c_0 $, comme l'état caché initial est initialisé à un tenseur égal à zéro. Cependant, la prédiction de sentiment n'est faite qu'en utilisant l'état caché final, pas l'état final de la cellule, c'est-à-dire $ \\hat {y} = f (h_T) $.\n",
    " - Bidirectionnel : en plus d'avoir un LSTM traitant les mots de la phrase du premier au dernier (un LSTM forward), nous avons un deuxième LSTM traitant les mots de la phrase du dernier au premier (un LSTM backward). Au pas de temps $t$, le LSTM forward traite le mot $ x_t $, et le LSTM backward traite le mot $ x_ {T-t + 1} $.\n",
    " - Multi-couches : l'idée est que nous ajoutons des LSTM supplémentaires en plus du LSTM standard initial, où chaque LSTM ajouté est une autre couche. La sortie d'état caché par le premier LSTM (inférieur) au pas de temps $t$ sera l'entrée du LSTM au-dessus de lui au pas de temps $ t $. La prédiction est alors faite à partir de l'état caché final de la couche finale (la plus haute).\n",
    " - Régularisation : plus on a de paramètres dans notre modèle, plus la probabilité que notre modèle se sur-ajustement est élevée (mémoriser les données d'entraînement, provoquant une erreur d'entraînement faible mais une erreur de validation / test élevée, c'est-à-dire une mauvaise généralisation à de nouveaux exemples). Pour lutter contre cela, nous utilisons la régularisation. Plus précisément, nous utilisons une méthode de régularisation appelée dropout. Le dropout fonctionne en supprimant aléatoirement (mise à 0) les neurones dans une couche lors d'un passage vers l'avant.\n",
    " - Avant de transmettre les embeddings au LSTM, nous devons les emballer, ce que nous faisons avec nn.utils.rnn.packed_padded_sequence. Cela obligera notre LSTM à ne traiter que les éléments non rembourrés de notre séquence. Le LSTM retournera alors packed_output (une séquence condensée) ainsi que les états caché et cellule (qui sont tous deux des tenseurs). Sans séquences remplies remplies, masqué et cellule sont des tenseurs du dernier élément de la séquence, qui sera très probablement un token de remplissage, mais lorsque vous utilisez des séquences remplies de remplissage, ils proviennent tous deux du dernier élément non rempli de la séquence. Nous décompressons ensuite la séquence de sortie, avec nn.utils.rnn.pad_packed_sequence, pour la transformer d'une séquence compressée en un tenseur. Les éléments de sortie des tokens de remplissage seront des tenseurs nuls (tenseurs où chaque élément est nul). Habituellement, nous n'avons à décompresser la sortie que si nous voulons l'utiliser plus tard dans le modèle. \n",
    " \n",
    " \n",
    "L'état caché final, caché, a une forme de **[nombre de couches * nombre de directions, taille de lot, caché dim]**. Ceux-ci sont classés: **[forward_layer_0, backward_layer_0, forward_layer_1, backward_layer 1, ..., forward_layer_n, backward_layer n]**. Comme nous voulons les états cachés avant et arrière de la couche finale (supérieure), nous obtenons les deux couches cachées supérieures de la première dimension, cachées [-2,:,:] et cachées [-1,:,:], et les concaténons ensemble avant de les transmettre au calque linéaire (après application de la suppression).\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        #pack sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        \n",
    "        #unpack sequence\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #output over padding tokens are zero tensors\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        #and apply dropout\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "                \n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "            \n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme auparavant, nous allons créer une instance de notre classe RNN, avec les nouveaux paramètres et arguments pour le nombre de couches, la bidirectionnalité et la probabilité de dropout.\n",
    "\n",
    "Pour garantir que les vecteurs pré-entraînés peuvent être chargés dans le modèle, EMBEDDING_DIM doit être égal à celui des vecteurs GloVe pré-entraînés chargés précédemment.\n",
    "\n",
    "Nous obtenons notre index de jeton de pad à partir du vocabulaire, obtenant la chaîne réelle représentant le jeton de pad à partir de l'attribut pad_token du champ, qui est <pad> par défaut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 4,810,857 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dernier ajout consiste à copier les embeddings de mots pré-entraînées que nous avons chargées précédemment dans la couche d'embedding de notre modèle.\n",
    "\n",
    "Nous récupérons les embeddings à partir du vocabulaire de Field et vérifions qu'elles sont de la bonne taille, **[taille du vocabulaire,dim embedding]**\n",
    "\n",
    "Comme nos tokens <unk> et <pad> ne sont pas dans le vocabulaire pré-entraîné, ils ont été initialisés en utilisant unk_init (une distribution $ \\mathcal {N} (0,1) $) lors de la construction de notre vocabulaire. Il est préférable de les initialiser tous les deux à zéros pour indiquer explicitement à notre modèle que, au départ, ils ne sont pas pertinents pour déterminer le sentiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25002, 100])\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
      "        ...,\n",
      "        [ 2.2529,  0.8734, -0.3372,  ...,  0.8701, -0.1612,  0.4326],\n",
      "        [-1.5441,  1.0105,  0.9192,  ...,  1.0422,  0.3977, -0.7357],\n",
      "        [-0.4597,  1.4756,  1.1808,  ...,  1.5027, -0.8486,  0.6017]])\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "print(pretrained_embeddings.shape)\n",
    "\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "print(model.embedding.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def precision(preds, y):\n",
    "    '''\n",
    "    Retourne la précision\n",
    "    '''\n",
    "    y_pred = torch.round(torch.sigmoid(preds))\n",
    "    y_true = (y_pred == y).float() \n",
    "            \n",
    "    tp = (y_true * y_pred).sum().float()\n",
    "    tn = ((1 - y_true) * (1 - y_pred)).sum().float()\n",
    "    fp = ((1 - y_true) * y_pred).sum().float()\n",
    "    fn = (y_true * (1 - y_pred)).sum().float()\n",
    "       \n",
    "    precision = tp / (tp + fp)\n",
    "    \n",
    "    return precision\n",
    "\n",
    "def recall(preds, y):\n",
    "    '''\n",
    "    Retourne le recall\n",
    "    '''\n",
    "    y_pred = torch.round(torch.sigmoid(preds))\n",
    "    y_true = (y_pred == y).float()       \n",
    "    \n",
    "    tp = (y_true * y_pred).sum().float()\n",
    "    tn = ((1 - y_true) * (1 - y_pred)).sum().float()\n",
    "    fp = ((1 - y_true) * y_pred).sum().float()\n",
    "    fn = (y_true * (1 - y_pred)).sum().float()\n",
    "    recall = tp / (tp + fn)\n",
    "    return recall\n",
    "\n",
    "\n",
    "def f1_loss(preds, y):\n",
    "    '''\n",
    "    Retourne le score F1\n",
    "    '''  \n",
    "    y_pred = torch.round(torch.sigmoid(preds))\n",
    "    y_true = (y_pred == y).float() \n",
    "            \n",
    "    tp = (y_true * y_pred).sum().float()\n",
    "    tn = ((1 - y_true) * (1 - y_pred)).sum().float()\n",
    "    fp = ((1 - y_true) * y_pred).sum().float()\n",
    "    fn = (y_true * (1 - y_pred)).sum().float()\n",
    "    \n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    \n",
    "    f1 = 2* (precision*recall) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimiseur et fonction de perte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraîner le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, text_lengths = batch.text\n",
    "        \n",
    "        predictions = model(text, text_lengths).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            text, text_lengths = batch.text\n",
    "            \n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    \n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load('tut2-model.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tester le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Notre fonction predict_sentiment fait plusieurs choses:\n",
    "\n",
    " - définit le modèle en mode d'évaluation\n",
    " - tokenise la phrase, c'est-à-dire la divise d'une chaîne brute en une liste de jetons\n",
    " - indexe les jetons en les convertissant en leur représentation entière à partir de notre vocabulaire\n",
    " - obtient la longueur de notre séquence\n",
    " - convertit les index, qui sont une liste Python en un tenseur PyTorch\n",
    " - ajouter une dimension de lot en relâchant\n",
    " - convertit la longueur en un tenseur\n",
    " - écrase la prédiction de sortie d'un nombre réel compris entre 0 et 1 avec la fonction sigmoïde\n",
    " - convertit le tenseur contenant une valeur unique en un entier avec la méthode item ()\n",
    "\n",
    "Nous nous attendons à ce que les avis avec un sentiment négatif renvoient une valeur proche de 0 et les avis positifs renvoient une valeur proche de 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentiment(model, \"This film is great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentiment(model, \"This film is terrible\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
