{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import string\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Étape 1 : Importer les données\n",
    "\n",
    "On importe les données déjà pré-traitées et sous forme de DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Tidy_Reviews</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5736</th>\n",
       "      <td>I have yesterday seen the second part. And I m...</td>\n",
       "      <td>yesterday second part must say actually well f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7630</th>\n",
       "      <td>The plot of this film is not strong at all, lo...</td>\n",
       "      <td>plot strong lot hole approach car bad lot grea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8208</th>\n",
       "      <td>This is a great film. Touching and strong. The...</td>\n",
       "      <td>great touch strong direction without question ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5863</th>\n",
       "      <td>This was the best documentary I've ever seen!!...</td>\n",
       "      <td>best documentary ever saw lord dogtown want kn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6231</th>\n",
       "      <td>Very disappointing version of Lorna Doone. Too...</td>\n",
       "      <td>disappoint version lorna doone many miss roman...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1823</th>\n",
       "      <td>I do not envy Barry Levinson, Rachel Weisz, Be...</td>\n",
       "      <td>envy barry levinson rachel weisz ben stiller j...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3719</th>\n",
       "      <td>While rehearing Carmen of Bizet, the middle-ag...</td>\n",
       "      <td>rehear carmen bizet choreographer antonio anto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5523</th>\n",
       "      <td>This independent, B&amp;W, DV feature consistently...</td>\n",
       "      <td>independent dv feature consistently shock amaz...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>This is Paul F. Ryan's first and only full-len...</td>\n",
       "      <td>paul ryan first feature do anything since howe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5921</th>\n",
       "      <td>This game is one of the best RPG. Fist, It is ...</td>\n",
       "      <td>game best rpg fist actually amuse battle syste...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Reviews  \\\n",
       "5736  I have yesterday seen the second part. And I m...   \n",
       "7630  The plot of this film is not strong at all, lo...   \n",
       "8208  This is a great film. Touching and strong. The...   \n",
       "5863  This was the best documentary I've ever seen!!...   \n",
       "6231  Very disappointing version of Lorna Doone. Too...   \n",
       "1823  I do not envy Barry Levinson, Rachel Weisz, Be...   \n",
       "3719  While rehearing Carmen of Bizet, the middle-ag...   \n",
       "5523  This independent, B&W, DV feature consistently...   \n",
       "63    This is Paul F. Ryan's first and only full-len...   \n",
       "5921  This game is one of the best RPG. Fist, It is ...   \n",
       "\n",
       "                                           Tidy_Reviews  label  \n",
       "5736  yesterday second part must say actually well f...      1  \n",
       "7630  plot strong lot hole approach car bad lot grea...      1  \n",
       "8208  great touch strong direction without question ...      0  \n",
       "5863  best documentary ever saw lord dogtown want kn...      0  \n",
       "6231  disappoint version lorna doone many miss roman...      1  \n",
       "1823  envy barry levinson rachel weisz ben stiller j...      1  \n",
       "3719  rehear carmen bizet choreographer antonio anto...      0  \n",
       "5523  independent dv feature consistently shock amaz...      0  \n",
       "63    paul ryan first feature do anything since howe...      0  \n",
       "5921  game best rpg fist actually amuse battle syste...      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_pickle(\"clean_data.pkl\")\n",
    "# mélanger les données\n",
    "dataset = dataset.sample(frac=1)\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On split les données en données d'entraînement, de test et de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tTaille des features:\n",
      "Train set: \t\t(18163,) \n",
      "Validation set: \t(2271,) \n",
      "Test set: \t\t(2270,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "reviews_train, reviews_test, label_train, label_test = train_test_split(dataset[\"Tidy_Reviews\"], dataset[\"label\"], test_size=0.2, random_state=42)\n",
    "data_test, data_val, y_test, y_val = train_test_split(reviews_test, label_test, test_size=0.5, random_state=42)\n",
    "\n",
    "data_test = data_test.to_numpy()\n",
    "data_val = data_val.to_numpy()\n",
    "data_train = reviews_train.to_numpy()\n",
    "\n",
    "print(\"\\t\\t\\tTaille des features:\")\n",
    "print(\"Train set: \\t\\t{}\".format(reviews_train.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(data_val.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(data_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Étape 2 : Pré-traitement des données\n",
    "\n",
    "## Créer un vocabulaire et encoder les commentaires\n",
    "\n",
    "On utilise seulement le train pour créer le vocabulaire et si un mot du test n'est pas dans le vocabulaire, on le remplacera par \"unk\" comme unknown.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk    \n",
    "voc = Counter()\n",
    "for review in data_train:\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    voc.update(tokens)\n",
    "\n",
    "voc.update([\"<unk>\"]) \n",
    "\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(voc, 1)}\n",
    "\n",
    "def encode(data):\n",
    "    reviews_ints = [] \n",
    "    for review in data:\n",
    "        reviews_ints.append([vocab_to_int[word] if word in voc else vocab_to_int[\"<unk>\"] for word in review.split()])\n",
    "    return reviews_ints\n",
    "\n",
    "encoded_reviews = encode(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mots uniques:  54122\n",
      "Commentaire: \n",
      " [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 17, 18, 30, 31, 32, 33, 25, 34, 35, 36, 37, 38, 27, 39, 27, 40, 41, 42, 43, 44, 27, 45, 46, 47, 14, 45, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 22, 60, 61, 62, 63, 27, 60, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 27, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 25, 113, 114, 115, 116, 112, 117, 118, 119, 96, 120, 121, 122, 123, 28, 124, 125, 126, 127, 128, 22, 129, 130, 131, 132, 133, 116, 134, 135, 136, 28, 137, 113, 138, 139, 140, 141, 142, 143, 8, 96, 144, 145, 146, 147, 148, 149, 150, 27, 151, 152, 153, 154]]\n"
     ]
    }
   ],
   "source": [
    "print('Mots uniques: ', len((vocab_to_int)))\n",
    "print('Commentaire: \\n', encoded_reviews[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supprimer les commentaires de longueurs nulles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de ommentaires de longueur nulle: 0\n",
      "Longueur maximale pour un commentaire: 1360\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "review_lens = Counter([len(x) for x in encoded_reviews])\n",
    "print(\"Nombre de ommentaires de longueur nulle: {}\".format(review_lens[0]))\n",
    "print(\"Longueur maximale pour un commentaire: {}\".format(max(review_lens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce jeu de données il n'y en a pas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faire en sorte d'avoir des commentaires de même longueur\n",
    "\n",
    "Lorsque nous introduisons les commentaires dans notre modèle, nous introduisons un batch de plusieurs commentaires à la fois, et tous les commentaires doivent avoir la même taille. Les commentaires plus courts que la taille donnée seq_length sont complétés par des 0, et les plus longs sont tronqués.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [ 326  361  362  328  363  364  365  366  367  124]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [ 637  638  639  640  641  642  643  285  644  469]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [1356  269 1357 1358 1359 1360  334  102 1361  647]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [  18 1571 1572 1573 1080 1089 1574  205 1575 1571]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [ 330  579   62   36  598  474 1228  598 1760 1761]\n",
      " [   0    0    0    0    0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "def pad_features(encoded_reviews, seq_length):\n",
    "    \n",
    "    features = np.zeros((len(encoded_reviews), seq_length), dtype=int)\n",
    "    for i, row in enumerate(encoded_reviews):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    return features\n",
    "\n",
    "seq_length = 200\n",
    "features = pad_features(encoded_reviews, seq_length=seq_length)\n",
    "\n",
    "## test ##\n",
    "assert len(features)==len(encoded_reviews)\n",
    "assert len(features[0])==seq_length\n",
    "\n",
    "# print first 10 values of the first 30 batches \n",
    "print(features[:30,:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-traitement des données de test et de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tTaille des datasets:\n",
      "Train set: \t\t(18163, 200) \n",
      "Validation set: \t(2271, 200) \n",
      "Test set: \t\t(2270, 200)\n"
     ]
    }
   ],
   "source": [
    "data_val_ints = encode(data_val)\n",
    "data_val = pad_features(data_val_ints, seq_length=seq_length)\n",
    "\n",
    "data_test_ints = encode(data_test)\n",
    "data_test = pad_features(data_test_ints, seq_length=seq_length)\n",
    "\n",
    "train_x = np.array(features)\n",
    "train_y = np.array(label_train)\n",
    "val_x = np.array(data_val)\n",
    "val_y = np.array(y_val)\n",
    "test_x = np.array(data_test)\n",
    "test_y = np.array(y_test)\n",
    "\n",
    "\n",
    "print(\"\\t\\t\\tTaille des datasets:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 200])\n",
      "Sample input: \n",
      " tensor([[    0,     0,     0,  ...,  1646,  1500,  2077],\n",
      "        [ 2063,  9816,    25,  ...,   757,  1265,    37],\n",
      "        [    0,     0,     0,  ...,  2501,    22,     2],\n",
      "        ...,\n",
      "        [    0,     0,     0,  ...,   237,  1468,  3644],\n",
      "        [ 1346,  3507,  2171,  ...,   930,  1859,   199],\n",
      "        [33194,   832,  5903,  ...,   868,  1540,    53]])\n",
      "\n",
      "Sample label size:  torch.Size([50])\n",
      "Sample label: \n",
      " tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
      "        0, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "# un batch du data train\n",
    "dataiter = iter(valid_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Étape 3 : Définir le modèle LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"lstm.png\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les couches du modèle sont les suivantes :\n",
    "\n",
    " - Une couche embedding qui convertit nos tokens (entiers) en embeddings d'une taille spécifique.\n",
    " - Une couche LSTM définie par une taille et un nombre de couches hidden_state\n",
    " - Une couche de sortie entièrement connectée qui mappe les sorties de la couche LSTM à la taille du output souhaitée\n",
    " - Une couche d'activation sigmoïde qui transforme toutes les sorties en une valeur de 0 à 1; renvoie uniquement la dernière sortie sigmoïde comme output de ce réseau.\n",
    " - Output: la sortie sigmoïde du dernier pas de temps est considérée comme la sortie finale de ce réseau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, n_hidden, n_output, n_layers, drop_p = 0.5):\n",
    "        super().__init__()\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        # params: \"n_\" means dimension\n",
    "        self.n_vocab = n_vocab     # number of unique words in vocabulary\n",
    "        self.n_layers = n_layers   # number of LSTM layers \n",
    "        self.n_hidden = n_hidden   # number of hidden nodes in LSTM\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        self.lstm = nn.LSTM(n_embed, n_hidden, n_layers, batch_first = True, dropout = drop_p)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward (self, input_words, hidden):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "             # INPUT   :  (batch_size, seq_length)\n",
    "        embedded_words = self.embedding(input_words)   # (batch_size, seq_length, n_embed)\n",
    "        lstm_out, h = self.lstm(embedded_words, hidden)         # (batch_size, seq_length, n_hidden)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.n_hidden) # (batch_size*seq_length, n_hidden)\n",
    "        fc_out = self.fc(lstm_out)                      # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = self.sigmoid(fc_out)              # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = sigmoid_out.view(batch_size, -1)  # (batch_size, seq_length*n_output)\n",
    "        \n",
    "        # extract the output of ONLY the LAST output of the LAST element of the sequence\n",
    "        sigmoid_last = sigmoid_out[:, -1]               # (batch_size, 1)\n",
    "        \n",
    "        return sigmoid_last, h\n",
    "    \n",
    "    \n",
    "    def init_hidden (self, batch_size):  # initialize hidden weights (h,c) to 0\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        weights = next(self.parameters()).data\n",
    "        h = (weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "             weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition des paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(vocab_to_int)\n",
    "n_embed = 400\n",
    "n_hidden = 512\n",
    "n_output = 1   # 1 (\"positive\") or 0 (\"negative\")\n",
    "n_layers = 2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = SentimentLSTM(n_vocab, n_embed, n_hidden, n_output, n_layers)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction de coût et optimiseur\n",
    "\n",
    "La fonction de coût mesure la Binary Cross Entropy entre le target le output, et on utilise l'optimiseur Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
    "\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3 Step: 100 Training Loss: 0.6691 Validation Loss: 0.6638\n",
      "Epoch: 1/3 Step: 200 Training Loss: 0.6891 Validation Loss: 0.6754\n",
      "Epoch: 1/3 Step: 300 Training Loss: 0.4935 Validation Loss: 0.4822\n",
      "Epoch: 2/3 Step: 400 Training Loss: 0.4593 Validation Loss: 0.4383\n",
      "Epoch: 2/3 Step: 500 Training Loss: 0.3841 Validation Loss: 0.3622\n",
      "Epoch: 2/3 Step: 600 Training Loss: 0.4043 Validation Loss: 0.3313\n",
      "Epoch: 2/3 Step: 700 Training Loss: 0.2427 Validation Loss: 0.1666\n",
      "Epoch: 3/3 Step: 800 Training Loss: 0.3912 Validation Loss: 0.4194\n",
      "Epoch: 3/3 Step: 900 Training Loss: 0.2728 Validation Loss: 0.2008\n",
      "Epoch: 3/3 Step: 1000 Training Loss: 0.5354 Validation Loss: 0.5287\n"
     ]
    }
   ],
   "source": [
    "print_every = 100\n",
    "step = 0\n",
    "n_epochs = 3  \n",
    "clip = 5  # for gradient clip to prevent exploding gradient problem in LSTM/RNN\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # making requires_grad = False for the latest set of h\n",
    "        h = tuple([each.data for each in h])   \n",
    "        \n",
    "        net.zero_grad()\n",
    "        output, h = net(inputs, h)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (step % print_every) == 0:            \n",
    "            ######################\n",
    "            ##### VALIDATION #####\n",
    "            ######################\n",
    "            net.eval()\n",
    "            valid_losses = []\n",
    "            v_h = net.init_hidden(batch_size)\n",
    "            \n",
    "            for v_inputs, v_labels in valid_loader:\n",
    "                v_inputs, v_labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "                v_h = tuple([each.data for each in v_h])\n",
    "                \n",
    "                v_output, v_h = net(v_inputs, v_h)\n",
    "                v_loss = criterion(v_output.squeeze(), v_labels.float())\n",
    "                valid_losses.append(v_loss.item())\n",
    "\n",
    "            print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "                  \"Step: {}\".format(step),\n",
    "                  \"Training Loss: {:.4f}\".format(loss.item()),\n",
    "                  \"Validation Loss: {:.4f}\".format(np.mean(valid_losses)))\n",
    "            net.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test sur les données test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f91712b4d48f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtest_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_h\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtest_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtest_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-4dba98c17b22>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_words, hidden)\u001b[0m\n\u001b[1;32m     22\u001b[0m              \u001b[0;31m# INPUT   :  (batch_size, seq_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0membedded_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_words\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# (batch_size, seq_length, n_embed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# (batch_size, seq_length, n_hidden)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mlstm_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mlstm_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_hidden\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (batch_size*seq_length, n_hidden)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 559\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net.eval()\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "test_h = net.init_hidden(batch_size)\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    test_h = tuple([each.data for each in test_h])\n",
    "    test_output, test_h = net(inputs,test_h)\n",
    "    loss = criterion(test_output, labels.float())\n",
    "    test_losses.append(loss.item())\n",
    "    \n",
    "    preds = torch.round(test_output.squeeze())\n",
    "    correct_tensor = preds.eq(labels.float().view_as(preds))\n",
    "    correct = np.squeeze(correct_tensor.numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "    \n",
    "print(\"Test Loss: {:.4f}\".format(np.mean(test_losses)))\n",
    "print(\"Test Accuracy: {:.2f}\".format(num_correct/len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test sur un commentaire nouveau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, review, seq_length = 200):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    test_ints = []\n",
    "    \n",
    "    for word in review:\n",
    "        review = review.lower() \n",
    "        test_text = ''.join([c for c in review if c not in punctuation])\n",
    "        test_words = test_text.split()    \n",
    "        test_ints.append([vocab_to_int[word] if word in voc else vocab_to_int[\"<unk>\"] for word in test_words ])\n",
    "        \n",
    "    padded_words = pad_features(test_ints, seq_length)\n",
    "    padded_words = torch.from_numpy(padded_words).to(device)\n",
    "    \n",
    "    if(len(padded_words) == 0):\n",
    "        \"Your review must contain at least 1 word!\"\n",
    "        return None\n",
    "    \n",
    "    net.eval()\n",
    "    h = net.init_hidden(1)\n",
    "    output, h = net(padded_words, h)\n",
    "    pred = torch.round(output.squeeze())\n",
    "    msg = \"This is a positive review.\" if pred == 0 else \"This is a negative review.\"\n",
    "    \n",
    "    return msg\n",
    "\n",
    "\n",
    "review1 = \"It made me cry.\"\n",
    "review2 = \"It was so good it made me cry.\"\n",
    "review3 = \"It's ok.\"\n",
    "review4 = \"This movie had the best acting and the dialogue was so good. I loved it.\"\n",
    "review5 = \"Garbage\"\n",
    "                       ### OUTPUT ###\n",
    "predict(net, review1)  ## negative ##\n",
    "predict(net, review2)  ## positive ##\n",
    "predict(net, review3)  ## negative ##\n",
    "predict(net, review4)  ## positive ##\n",
    "predict(net, review5)  ## negative ##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
