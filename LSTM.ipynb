{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import string\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etape 1 : Importer les données\n",
    "\n",
    "On importe les données déjà pré-traitées et sous forme de DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Tidy_Reviews</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5160</th>\n",
       "      <td>This movie was amusing at times, hell sometime...</td>\n",
       "      <td>amuse hell sometimes even downright underlie m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4607</th>\n",
       "      <td>I don't know about you, but what I love about ...</td>\n",
       "      <td>know love tom jerry cartoon often violent inte...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>I liked this movie a lot. It really intrigued ...</td>\n",
       "      <td>like lot really intrigue deanna alicia become ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7111</th>\n",
       "      <td>Who says zombies can't be converted into usefu...</td>\n",
       "      <td>say zombie ca convert useful member community ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6433</th>\n",
       "      <td>As of this writing John Carpenter's 'Halloween...</td>\n",
       "      <td>write john carpenter near anniversary since sp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8014</th>\n",
       "      <td>I own this movie and have watched it several t...</td>\n",
       "      <td>watch several throughout year since release pr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5465</th>\n",
       "      <td>This movie has some things that are pretty ama...</td>\n",
       "      <td>thing pretty amaze first suppose base true ama...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11131</th>\n",
       "      <td>This is one of the few movies I watched twice ...</td>\n",
       "      <td>watch twice theatre really love atmosphere tel...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5565</th>\n",
       "      <td>After watching two of his silent shorts, 'Elen...</td>\n",
       "      <td>watch two silent short men first french direct...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10983</th>\n",
       "      <td>Enjoyable in spite of Leslie Howard's performa...</td>\n",
       "      <td>enjoyable spite leslie howard performance howa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Reviews  \\\n",
       "5160   This movie was amusing at times, hell sometime...   \n",
       "4607   I don't know about you, but what I love about ...   \n",
       "42     I liked this movie a lot. It really intrigued ...   \n",
       "7111   Who says zombies can't be converted into usefu...   \n",
       "6433   As of this writing John Carpenter's 'Halloween...   \n",
       "8014   I own this movie and have watched it several t...   \n",
       "5465   This movie has some things that are pretty ama...   \n",
       "11131  This is one of the few movies I watched twice ...   \n",
       "5565   After watching two of his silent shorts, 'Elen...   \n",
       "10983  Enjoyable in spite of Leslie Howard's performa...   \n",
       "\n",
       "                                            Tidy_Reviews  label  \n",
       "5160   amuse hell sometimes even downright underlie m...      1  \n",
       "4607   know love tom jerry cartoon often violent inte...      1  \n",
       "42     like lot really intrigue deanna alicia become ...      0  \n",
       "7111   say zombie ca convert useful member community ...      0  \n",
       "6433   write john carpenter near anniversary since sp...      0  \n",
       "8014   watch several throughout year since release pr...      0  \n",
       "5465   thing pretty amaze first suppose base true ama...      0  \n",
       "11131  watch twice theatre really love atmosphere tel...      0  \n",
       "5565   watch two silent short men first french direct...      0  \n",
       "10983  enjoyable spite leslie howard performance howa...      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_pickle(\"clean_data.pkl\")\n",
    "# mélanger les données\n",
    "dataset = dataset.sample(frac=1)\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On split les données en données d'entraînement, de test et de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(18163,) \n",
      "Validation set: \t(2271,) \n",
      "Test set: \t\t(2270,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "reviews_train, reviews_test, label_train, label_test = train_test_split(dataset[\"Tidy_Reviews\"], dataset[\"label\"], test_size=0.2, random_state=42)\n",
    "data_test, data_val, y_test, y_val = train_test_split(reviews_test, label_test, test_size=0.5, random_state=42)\n",
    "\n",
    "data_test = data_test.to_numpy()\n",
    "data_val = data_val.to_numpy()\n",
    "data_train = reviews_train.to_numpy()\n",
    "\n",
    "print(\"\\t\\t\\tTaille des features:\")\n",
    "print(\"Train set: \\t\\t{}\".format(reviews_train.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(data_val.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(data_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etape 2 : Pré-traitement des données\n",
    "\n",
    "## Créer un vocabulaire et encoder les commentaires\n",
    "\n",
    "On utilise seulement le train pour créer le vocabulaire et si un mot du test n'est pas dans le vocabulaire, on le remplacera par \"unk\" comme unknown.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk    \n",
    "voc = Counter()\n",
    "for review in data_train:\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    voc.update(tokens)\n",
    "\n",
    "voc.update([\"<unk>\"]) \n",
    "\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(voc, 1)}\n",
    "def encode(data):\n",
    "    reviews_ints = [] \n",
    "    for review in data:\n",
    "        reviews_ints.append([vocab_to_int[word] if word in voc else vocab_to_int[\"<unk>\"] for word in review.split()])\n",
    "    return reviews_ints\n",
    "\n",
    "encoded_reviews = encode(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mots uniques:  54325\n",
      "Commentaire: \n",
      " [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1, 18, 19, 20, 21, 22, 23, 24, 25, 2, 26, 27, 28, 10, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 28, 43, 44, 45, 46, 47, 10, 48, 49, 50, 51, 52, 53, 48, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 32, 73, 74, 62, 12, 75, 76, 77, 1, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 21, 101, 102, 103, 104, 105, 83, 106, 44, 107, 108, 109, 110, 69, 53, 111, 112, 113, 114, 115, 116, 117, 63, 64, 41, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 63, 64, 131, 132, 133, 134, 17, 69, 53, 135, 32, 41, 136, 137, 138, 139, 140, 2, 18, 138, 53, 141, 142, 143, 140, 144, 145, 2, 146, 138, 147, 148, 149, 14, 150, 66, 1, 151, 152, 153, 154, 155, 156, 157, 158]]\n"
     ]
    }
   ],
   "source": [
    "print('Mots uniques: ', len((vocab_to_int)))\n",
    "print('Commentaire: \\n', encoded_reviews[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supprimer les commentaires de longueurs nulles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de ommentaires de longueur nulle: 0\n",
      "Longueur maximale pour un commentaire: 1360\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "review_lens = Counter([len(x) for x in encoded_reviews])\n",
    "print(\"Nombre de ommentaires de longueur nulle: {}\".format(review_lens[0]))\n",
    "print(\"Longueur maximale pour un commentaire: {}\".format(max(review_lens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce jeu de données il n'y en a pas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faire en sorte d'avoir des commentaires de même longueur\n",
    "\n",
    "Lorsque nous introduisons les commentaires dans notre modèle, nous introduisons un batch de plusieurs commentaires à la fois, et tous les commentaires doivent avoir la même taille. Les commentaires plus courts que la taille donnée seq_length sont complétés par des 0, et les plus longs sont tronqués.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    1    2    3]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [ 409  410  411  412  413  414    3  349  415  377]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [ 368  679  680  660  681  682  132  683  684  685]\n",
      " [ 929  930  931  932  933  934  338  935  936  937]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [1254   99  305 1255  138 1153 1256 1257 1258 1259]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [1426 1427 1428 1429 1430 1431 1227   44 1382 1432]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [1662  376 1663  678  291  603  722  726  418  381]\n",
      " [1762 1763  282  377 1764  626 1141 1765  994 1766]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "def pad_features(encoded_reviews, seq_length):\n",
    "    \n",
    "    features = np.zeros((len(encoded_reviews), seq_length), dtype=int)\n",
    "    for i, row in enumerate(encoded_reviews):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    return features\n",
    "\n",
    "seq_length = 200\n",
    "features = pad_features(encoded_reviews, seq_length=seq_length)\n",
    "\n",
    "## test ##\n",
    "assert len(features)==len(encoded_reviews), \n",
    "assert len(features[0])==seq_length, \n",
    "\n",
    "# print first 10 values of the first 30 batches \n",
    "print(features[:30,:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-traitement des données de test et de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(18163, 200) \n",
      "Validation set: \t(2271, 200) \n",
      "Test set: \t\t(2270, 200)\n"
     ]
    }
   ],
   "source": [
    "data_val_ints = encode(data_val)\n",
    "data_val = pad_features(data_val_ints, seq_length=seq_length)\n",
    "\n",
    "data_test_ints = encode(data_test)\n",
    "data_test = pad_features(data_test_ints, seq_length=seq_length)\n",
    "\n",
    "train_x = np.array(features)\n",
    "train_y = np.array(label_train)\n",
    "val_x = np.array(data_val)\n",
    "val_y = np.array(y_val)\n",
    "test_x = np.array(data_test)\n",
    "test_y = np.array(y_test)\n",
    "\n",
    "\n",
    "print(\"\\t\\t\\tTaille des datasets:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "# un batch du data train\n",
    "dataiter = iter(valid_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etape 3 : Définir le modèle LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"lstm.png\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les couches du modèle sont les suivantes :\n",
    "\n",
    " - An embedding layer that converts our word tokens (integers) into embeddings of a specific size.\n",
    " - An LSTM layer defined by a hidden_state size and number of layers\n",
    " - A fully-connected output layer that maps the LSTM layer outputs to a desired output_size\n",
    " - A sigmoid activation layer which turns all outputs into a value 0–1; return only the last sigmoid output as the output of this network.\n",
    " - Output: Sigmoid output from the last timestep is considered as the final output of this network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, n_hidden, n_output, n_layers, drop_p = 0.5):\n",
    "        super().__init__()\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        # params: \"n_\" means dimension\n",
    "        self.n_vocab = n_vocab     # number of unique words in vocabulary\n",
    "        self.n_layers = n_layers   # number of LSTM layers \n",
    "        self.n_hidden = n_hidden   # number of hidden nodes in LSTM\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        self.lstm = nn.LSTM(n_embed, n_hidden, n_layers, batch_first = True, dropout = drop_p)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward (self, input_words):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "             # INPUT   :  (batch_size, seq_length)\n",
    "        embedded_words = self.embedding(input_words)   # (batch_size, seq_length, n_embed)\n",
    "        lstm_out, h = self.lstm(embedded_words)         # (batch_size, seq_length, n_hidden)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.n_hidden) # (batch_size*seq_length, n_hidden)\n",
    "        fc_out = self.fc(lstm_out)                      # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = self.sigmoid(fc_out)              # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = sigmoid_out.view(batch_size, -1)  # (batch_size, seq_length*n_output)\n",
    "        \n",
    "        # extract the output of ONLY the LAST output of the LAST element of the sequence\n",
    "        sigmoid_last = sigmoid_out[:, -1]               # (batch_size, 1)\n",
    "        \n",
    "        return sigmoid_last, h\n",
    "    \n",
    "    \n",
    "    def init_hidden (self, batch_size):  # initialize hidden weights (h,c) to 0\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        weights = next(self.parameters()).data\n",
    "        h = (weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "             weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction de coût et optimiseur\n",
    "\n",
    "La fonction de coût mesure la Binary Cross Entropy entre le target le output, et on utilise l'optimiseur Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
    "\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(vocab_to_int)\n",
    "n_embed = 400\n",
    "n_hidden = 512\n",
    "n_output = 1   # 1 (\"positive\") or 0 (\"negative\")\n",
    "n_layers = 2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = SentimentLSTM(n_vocab, n_embed, n_hidden, n_output, n_layers)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:24: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3 Step: 100 Training Loss: 0.5945 Validation Loss: 0.5433\n",
      "Epoch: 1/3 Step: 200 Training Loss: 0.6408 Validation Loss: 0.5998\n",
      "Epoch: 1/3 Step: 300 Training Loss: 0.6829 Validation Loss: 0.6342\n",
      "Epoch: 2/3 Step: 400 Training Loss: 0.5039 Validation Loss: 0.4478\n",
      "Epoch: 2/3 Step: 500 Training Loss: 0.5034 Validation Loss: 0.5546\n",
      "Epoch: 2/3 Step: 600 Training Loss: 0.5206 Validation Loss: 0.4958\n",
      "Epoch: 2/3 Step: 700 Training Loss: 0.3956 Validation Loss: 0.3618\n",
      "Epoch: 3/3 Step: 800 Training Loss: 0.4699 Validation Loss: 0.3849\n",
      "Epoch: 3/3 Step: 900 Training Loss: 0.3982 Validation Loss: 0.3201\n",
      "Epoch: 3/3 Step: 1000 Training Loss: 0.4506 Validation Loss: 0.3510\n"
     ]
    }
   ],
   "source": [
    "print_every = 100\n",
    "step = 0\n",
    "n_epochs = 3  # validation loss increases from ~ epoch 3 or 4\n",
    "clip = 5  # for gradient clip to prevent exploding gradient problem in LSTM/RNN\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # making requires_grad = False for the latest set of h\n",
    "        h = tuple([each.data for each in h])   \n",
    "        \n",
    "        net.zero_grad()\n",
    "        output, h = net(inputs)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (step % print_every) == 0:            \n",
    "            ######################\n",
    "            ##### VALIDATION #####\n",
    "            ######################\n",
    "            net.eval()\n",
    "            valid_losses = []\n",
    "            v_h = net.init_hidden(batch_size)\n",
    "            \n",
    "            for v_inputs, v_labels in valid_loader:\n",
    "                v_inputs, v_labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "                v_h = tuple([each.data for each in v_h])\n",
    "                \n",
    "                v_output, v_h = net(v_inputs)\n",
    "                v_loss = criterion(v_output.squeeze(), v_labels.float())\n",
    "                valid_losses.append(v_loss.item())\n",
    "\n",
    "            print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "                  \"Step: {}\".format(step),\n",
    "                  \"Training Loss: {:.4f}\".format(loss.item()),\n",
    "                  \"Validation Loss: {:.4f}\".format(np.mean(valid_losses)))\n",
    "            net.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-fec44c86e1d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtest_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnum_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtest_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-5b7e59fe3f22>\u001b[0m in \u001b[0;36minit_hidden\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         h = (weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n\u001b[0m\u001b[1;32m     45\u001b[0m              weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "################  12. TEST THE TRAINED MODEL ON THE TEST SET  #################\n",
    "###############################################################################\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net.eval()\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "test_h = net.init_hidden(batch_size)\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    test_h = tuple([each.data for each in test_h])\n",
    "    test_output, test_h = net(inputs)\n",
    "    loss = criterion(test_output, labels.float())\n",
    "    test_losses.append(loss.item())\n",
    "    \n",
    "    preds = torch.round(test_output.squeeze())\n",
    "    correct_tensor = preds.eq(labels.float().view_as(preds))\n",
    "    correct = np.squeeze(correct_tensor.numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "    \n",
    "print(\"Test Loss: {:.4f}\".format(np.mean(test_losses)))\n",
    "print(\"Test Accuracy: {:.2f}\".format(num_correct/len(test_loader.dataset)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-8d5f9de68e2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mreview5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Garbage\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                        \u001b[0;31m### OUTPUT ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreview1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## negative ##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreview2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## positive ##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreview3\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## negative ##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-8d5f9de68e2b>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(net, review, seq_length)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mencoded_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvocab_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpadded_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoded_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocess' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "###############################################################################\n",
    "############  13. TEST THE TRAINED MODEL ON A RANDOM SINGLE REVIEW ############\n",
    "###############################################################################\n",
    "def predict(net, review, seq_length = 200):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    words = preprocess(review)\n",
    "    encoded_words = [vocab_to_int[word] for word in words]\n",
    "    padded_words = pad_text([encoded_words], seq_length)\n",
    "    padded_words = torch.from_numpy(padded_words).to(device)\n",
    "    \n",
    "    if(len(padded_words) == 0):\n",
    "        \"Your review must contain at least 1 word!\"\n",
    "        return None\n",
    "    \n",
    "    net.eval()\n",
    "    h = net.init_hidden(1)\n",
    "    output, h = net(padded_words, h)\n",
    "    pred = torch.round(output.squeeze())\n",
    "    msg = \"This is a positive review.\" if pred == 0 else \"This is a negative review.\"\n",
    "    \n",
    "    return msg\n",
    "\n",
    "\n",
    "review1 = \"It made me cry.\"\n",
    "review2 = \"It was so good it made me cry.\"\n",
    "review3 = \"It's ok.\"\n",
    "review4 = \"This movie had the best acting and the dialogue was so good. I loved it.\"\n",
    "review5 = \"Garbage\"\n",
    "                       ### OUTPUT ###\n",
    "predict(net, review1)  ## negative ##\n",
    "predict(net, review2)  ## positive ##\n",
    "predict(net, review3)  ## negative ##\n",
    "predict(net, review4)  ## positive ##\n",
    "predict(net, review5)  ## negative ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
